{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pickle\n",
    "import requests\n",
    "# import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import xlrd\n",
    "import multiprocessing\n",
    "\n",
    "import random\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Extract function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigunihan_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_bigUnihan(filename):\n",
    "    df = pd.read_csv(filename, sep='|', low_memory=False)\n",
    "    df = df[['char','kFrequency']].set_index('char')\n",
    "    df = df[~pd.isnull(df.kFrequency)]\n",
    "    \n",
    "    return df.to_dict()['kFrequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unihan.csv (注音跟倉頡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_unihan(unihan_filename):\n",
    "    global dicBPMF, dicPhone, dicCangjie, dicCangjie2char\n",
    "    with open(unihan_filename, 'r', encoding='utf8') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in spamreader:\n",
    "            row = [cell for cell in row] # unicode\n",
    "            char, bpmf, cangjie, components, jp, kr, name, pinyin_chs, pinyin_cht, char_strokes_count, radical, radical_name, radical_strokes_count = row\n",
    "            for ph in bpmf.split(): # 發音\n",
    "                dicBPMF[char] += [ph]\n",
    "                dicPhone[ph] += [char]\n",
    "            for cj in cangjie.split(): # 倉頡碼\n",
    "                if u\"難\" in cj: continue\n",
    "                for i in range(0, 3):\n",
    "                    if i == len(cj): continue\n",
    "                    # dicBPMF[char]['cangjie'] += [cj[:i]]\n",
    "                    dicCangjie[char] += [cj[i:]]\n",
    "                    # dicCangjie[cj[:i]] += [char]\n",
    "                    dicCangjie2char[cj[i:]] += [char]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unihan: sound similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_extract_same(char):\n",
    "    '''\n",
    "    Same neutral and tone\n",
    "    '''\n",
    "    return list(set(ch for ph in dicBPMF[char] for ch in dicPhone[ph]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sound_extract_tone(char):\n",
    "    '''\n",
    "    the char of different tone \n",
    "    '''\n",
    "    output = set()\n",
    "    tones = ['ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    for ph in dicBPMF[char]:\n",
    "        if ph[-1] in tones:\n",
    "            for t in tones:\n",
    "                if t == ph[-1]: continue\n",
    "                output = output.union(dicPhone[ph[:-1]+t])\n",
    "        else:\n",
    "            for t in tones:\n",
    "                output = output.union(dicPhone[ph+t])\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sound_extract_finalConsonant(char, toneKeep=True):\n",
    "    '''\n",
    "    單：ㄚㄛㄜㄝ\n",
    "    複：ㄞㄟㄠㄡ\n",
    "    鼻：ㄢㄣㄤㄥ\n",
    "    捲舌：ㄦ\n",
    "    '''\n",
    "    output = set()\n",
    "    tones = ['','ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    consonants = [\n",
    "        ['ㄚ','ㄛ','ㄜ','ㄝ'],\n",
    "        ['ㄞ','ㄟ','ㄠ','ㄡ'],\n",
    "        ['ㄢ','ㄣ','ㄤ','ㄥ']\n",
    "    ]\n",
    "    \n",
    "    for ph in dicBPMF[char]:\n",
    "        # Tone delete\n",
    "        if ph[-1] in tones:\n",
    "            neutral, tone = ph[:-1], ph[-1]\n",
    "        else:\n",
    "            neutral, tone = ph,''\n",
    "        \n",
    "        # Add relevent consonant\n",
    "        for cons in consonants:\n",
    "            if neutral[-1] in cons:\n",
    "                new_neutrals = set(neutral[:-1] + c for c in cons if c!=neutral[-1])\n",
    "                for n in new_neutrals:\n",
    "                    if toneKeep:\n",
    "                        output = output.union(dicPhone[n+tone])\n",
    "                    else:\n",
    "                        for t in tones:\n",
    "                            output = output.union(dicPhone[n+t])\n",
    "                break\n",
    "                               \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_extract_similartConsonant(char, toneKeep=True):\n",
    "    '''\n",
    "    一次只針對一種，不會並用\n",
    "    Initial\n",
    "    ㄈㄏ\n",
    "    ㄋㄌ\n",
    "    ㄓㄗ\n",
    "    ㄔㄘ\n",
    "    Final:\n",
    "    ㄢㄤ\n",
    "    ㄜㄦ\n",
    "    ㄣㄥ\n",
    "    Intermediate:\n",
    "    ㄧㄩ\n",
    "    '''\n",
    "    new_neutrals = set()\n",
    "    output = set()\n",
    "    tones = ['','ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    initial_pairs = [\n",
    "        ['ㄈ','ㄏ'],\n",
    "        ['ㄋ','ㄌ'],\n",
    "        ['ㄓ','ㄗ'],\n",
    "        ['ㄔ','ㄘ']\n",
    "    ]\n",
    "    final_pairs = [\n",
    "        ['ㄢ','ㄤ'],\n",
    "        ['ㄜ','ㄦ'],\n",
    "        ['ㄣ','ㄥ']\n",
    "    ]\n",
    "    inter_pairs = [['ㄧ','ㄩ']]\n",
    "    \n",
    "    for ph in dicBPMF[char]:        \n",
    "        # Tone delete\n",
    "        if ph[-1] in tones:\n",
    "            neutral, tone = ph[:-1], ph[-1]\n",
    "        else:\n",
    "            neutral, tone = ph, ''\n",
    "            \n",
    "        # Initial-consonant, just pick one \n",
    "        for cons in initial_pairs:\n",
    "            if neutral[0] in cons:\n",
    "                new_neutrals = new_neutrals.union(c + neutral[1:] + tone for c in cons if c!=neutral[0])\n",
    "                break        \n",
    "        \n",
    "                    \n",
    "        # Final-consonant       \n",
    "        for cons in final_pairs:\n",
    "#             print(neutral[-1], cons)\n",
    "            if neutral[-1] in cons:\n",
    "#                 print('i', cons)\n",
    "#                 print(neutral[:-1])\n",
    "#                 print(list(neutral[:-1] + c for c in cons if c!=neutral[-1]))\n",
    "                new_neutrals = new_neutrals.union(neutral[:-1] + c + tone for c in cons if c!=neutral[-1])\n",
    "                break\n",
    "        \n",
    "        # Inter_\n",
    "        for cons in inter_pairs:\n",
    "            for idx, tmp in enumerate(neutral):\n",
    "                if tmp in cons:\n",
    "                    new_neutrals = new_neutrals.union(neutral[:idx] + c + neutral[idx+1:] + tone for c in cons if c!=tmp)\n",
    "                    break\n",
    "    \n",
    "    ######## fIX TOne pRoblEm\n",
    "#     print(new_neutrals)\n",
    "    # Get candidate based on new_neutrals\n",
    "    for n in new_neutrals:\n",
    "        if toneKeep:\n",
    "            output = output.union(dicPhone[n])\n",
    "        else:\n",
    "            tmp = n[:-1] if n[-1] in tones else n            \n",
    "            for t in tones:\n",
    "                output = output.union(dicPhone[n+t])\n",
    "#         print(n,' '.join(output))\n",
    "        \n",
    "#     print(len(output))                     \n",
    "    return output if len(output)>0 else []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unihan: same cangjie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# No use\n",
    "def cangjie_extract_same(char):\n",
    "    cang = dicCangjie[char]\n",
    "    if len(cang) > 0:\n",
    "        output = set(dicCangjie2char[cang[0]])\n",
    "        output.remove(char)\n",
    "    else:\n",
    "        output = set()\n",
    "    \n",
    "    return list(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zwt.titles.txt (字典)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_zwtTitle(lines):\n",
    "    d = defaultdict(lambda: 0)\n",
    "    for word in lines:\n",
    "        d[word.strip()] += 1\n",
    "    #d[word.strip().decode('utf-8')[:2]] += 1\n",
    "    #print word.strip().decode('utf-8')[:2]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## radical.txt (部首)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def radicalDic(lines):\n",
    "    dicRadicalnum = defaultdict(list)\n",
    "    dicRadical = defaultdict(list)\n",
    "    for line in lines:\n",
    "        for char in line[5:].strip().split('|'):\n",
    "            dicRadical[char] += [line[:4]]\n",
    "            dicRadicalnum[line[:4]] += [char]\n",
    "    return dicRadicalnum, dicRadical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_similar(char):\n",
    "    return list(set(ch for rnum in dicRadical[char] for ch in dicRadicalnum[rnum]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error_correct pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_pairs(filelist):\n",
    "    for filename, path in filelist.items():\n",
    "        print('== Filename: {}'.format(filename))\n",
    "        '''\n",
    "        QQQQQQ 有兩個以上的錯誤在一個詞裡面，但更正只有一項\n",
    "        把上方例子放棄不取\n",
    "        有 duplicate \n",
    "        '''\n",
    "        # 1新編常用錯別字門診.txt OR 4教育部錯別字表.txt\n",
    "        if filename.startswith('1') or filename.startswith('4'):\n",
    "            df = pd.read_csv(path, sep='\\t')\n",
    "        # 2東東錯別字.txt OR 3常見錯別字一覽表.txt\n",
    "        elif filename.startswith('2') or filename.startswith('3'):        \n",
    "            df = pd.read_csv(path, sep='\\t', header=None, names = ['正確詞','錯誤詞','正確字','錯誤字'])\n",
    "        elif filename.startswith('udn_common'):\n",
    "            table = xlrd.open_workbook(path).sheet_by_index(0)\n",
    "            ch_dict = defaultdict(set)\n",
    "            # Have multierros (error_word to correct_word)\n",
    "            word_dict = defaultdict(set)\n",
    "            for idx in range(1,table.nrows):\n",
    "                row = table.row_values(idx)[:5]\n",
    "                # Consider the priority of pairs \n",
    "                if row[2].strip():            \n",
    "                    chs = row[2].split()\n",
    "                    if len(chs)==1:\n",
    "                        continue\n",
    "                    for i in range(1,len(chs)):\n",
    "                        freq = row[1] if type(row[1])==float else 1.0                \n",
    "                        ch_dict[chs[i]].add((int(freq),chs[0]))\n",
    "                elif row[3].strip():\n",
    "                    corr_seq = row[3].strip()\n",
    "                    error_seq = row[4].strip()\n",
    "                    word_dict[error_seq] = corr_seq\n",
    "            yield (filename, ch_dict, word_dict)\n",
    "            continue\n",
    "        elif filename.startswith('udn_pairs'):\n",
    "            ch_dict = defaultdict(set)\n",
    "            with open(path, 'r', encoding='utf8') as fp:\n",
    "                for line in fp:\n",
    "                    tt = line.split()\n",
    "                    if int(tt[2])>10:\n",
    "                        ch_dict[tt[0]].add((int(tt[2]), tt[1]))\n",
    "                    \n",
    "            yield (filename, ch_dict, dict())\n",
    "            continue\n",
    "        \n",
    "        print(filename)\n",
    "        \n",
    "        # For 1,2,3,4\n",
    "        if len(df)>0:\n",
    "            df = df.dropna()\n",
    "            df['idx'] = df.apply(lambda x:x['錯誤詞'].find(x['錯誤字']), axis=1)\n",
    "            df['pair'] = tuple(zip(df['idx'], df['錯誤字']))\n",
    "            df['noMultiErrors'] = df.apply(lambda x:x['正確詞']==x['錯誤詞'].replace(x['錯誤字'],x['正確字']), axis=1)\n",
    "            \n",
    "            # Remove multi-errors for the lack of right answer \n",
    "            preCnt = len(df)\n",
    "            df = df[df['noMultiErrors']==True]\n",
    "            postCnt = len(df)\n",
    "            \n",
    "            print('Original:{}\\tPost:{}'.format(preCnt,postCnt))\n",
    "            \n",
    "            df = df.set_index('錯誤詞')\n",
    "            \n",
    "            # Output DICT{'error_word':'(idx, corr_ch)'}\n",
    "#             df_slice = df[['pair']]\n",
    "#             word_dict = df_slice.to_dict()['pair']\n",
    "            word_dict = df[['正確詞']].to_dict()['正確詞']\n",
    "\n",
    "            # output DICT{'error_ch':set(cands)}\n",
    "            ch_dict = defaultdict(lambda :set())\n",
    "            pairs = tuple(zip(df['錯誤字'], df['正確字']))\n",
    "            for error_ch, corr_ch in pairs:\n",
    "                ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "            yield (filename, ch_dict, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Sentnece \n",
    "\n",
    "1. Bakeoff-2013 not work\n",
    "2. sequence error not append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_sentences(filelist):\n",
    "    for filename, path in filelist.items():\n",
    "        print('== Filename: {}'.format(filename))\n",
    "        \n",
    "        with open(path,'r',encoding='utf8') as fp:\n",
    "            soup = BeautifulSoup(fp, 'lxml')\n",
    "        \n",
    "        ch_dict = defaultdict(set)\n",
    "        word_dict = defaultdict(set)\n",
    "        seq_dict  = defaultdict(set)\n",
    "        \n",
    "        # Different label\n",
    "        if filename.startswith('Bakeoff'):\n",
    "            pass \n",
    "            ############## NOT FIX\n",
    "            for idx,element in enumerate(soup.find_all('DOC')):  \n",
    "                # Text\n",
    "                text = dict()\n",
    "                for pas in element.find('p').find_all('passage'):\n",
    "                    text[pas.get('id')] = pas.string\n",
    "\n",
    "                # Mistake\n",
    "                for mistake in element.find_all('mistake'):\n",
    "                    mis_id = mistake.get('id')\n",
    "                    mis_loc = mistake.get('location')\n",
    "                    mis_wrong = mistake.find('wrong').string.strip()\n",
    "                    mis_corr  = mistake.find('correction').string.strip()\n",
    "                    cur_seq = text.get(mis_id, '')\n",
    "\n",
    "                    pairs =  [(mis_wrong,idx,x,y) for idx, (x,y) in enumerate(zip(mis_wrong, mis_corr)) if x!=y]\n",
    "\n",
    "                    # error-corr\n",
    "                    for mis_wrong,idx,error_ch,corr_ch in pairs:\n",
    "                        # char-based\n",
    "                        ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "                        # word-based\n",
    "                        word_dict[mis_wrong].add((idx,corr_ch))\n",
    "\n",
    "        else:            \n",
    "            for idx,element in enumerate(soup.find_all('essay')):  \n",
    "                # Text\n",
    "                text = dict()\n",
    "                for pas in element.find('text').find_all('passage'):\n",
    "                    text[pas.get('id')] = pas.string\n",
    "\n",
    "                # Mistake\n",
    "                for mistake in element.find_all('mistake'):\n",
    "                    mis_id = mistake.get('id')\n",
    "                    mis_loc = mistake.get('location')\n",
    "                    mis_wrong = mistake.find('wrong').string.strip()\n",
    "                    mis_corr  = mistake.find('correction').string.strip()\n",
    "                    cur_seq = text.get(mis_id, '')\n",
    "\n",
    "                    pairs =  [(mis_wrong,idx,x,y) for idx, (x,y) in enumerate(zip(mis_wrong, mis_corr)) if x!=y]\n",
    "\n",
    "                    # error-corr\n",
    "                    for mis_wrong,idx,error_ch,corr_ch in pairs:\n",
    "                        # char-based\n",
    "                        ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "                        # word-based\n",
    "                        word_dict[mis_wrong].add((idx,corr_ch))\n",
    "\n",
    "                        # sequence-based \n",
    "                        ### Have problem with multiple errors in single word \n",
    "            #             seq_dict[cur_seq].add((int(mis_loc)-1,corr_ch))\n",
    "    \n",
    "        yield (filename, ch_dict, word_dict, seq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataroot = 'G:/UDN/training_confusion/{}/'.format\n",
    "dataroot = '/home/kiwi/udn_data/training_confusion/{}/'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Char information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['捰',\n",
       " '婀',\n",
       " '我',\n",
       " '猗',\n",
       " '僫',\n",
       " '阿',\n",
       " '騀',\n",
       " '娿',\n",
       " '悪',\n",
       " '婐',\n",
       " '婑',\n",
       " '硪',\n",
       " '惡',\n",
       " '噁',\n",
       " '頋',\n",
       " '厄',\n",
       " '妸']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_extract_same('我')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'儞',\n",
       " '尒',\n",
       " '尓',\n",
       " '尔',\n",
       " '峏',\n",
       " '栮',\n",
       " '洱',\n",
       " '爾',\n",
       " '珥',\n",
       " '耳',\n",
       " '薾',\n",
       " '迩',\n",
       " '邇',\n",
       " '鉺',\n",
       " '餌',\n",
       " '駬'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_extract_similartConsonant('我')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'㗁',\n",
       " '㮙',\n",
       " '䄉',\n",
       " '俄',\n",
       " '偓',\n",
       " '偔',\n",
       " '僫',\n",
       " '匎',\n",
       " '卧',\n",
       " '卾',\n",
       " '厄',\n",
       " '吪',\n",
       " '呃',\n",
       " '咢',\n",
       " '咹',\n",
       " '哦',\n",
       " '唈',\n",
       " '啐',\n",
       " '啞',\n",
       " '喔',\n",
       " '嗌',\n",
       " '噁',\n",
       " '噩',\n",
       " '囮',\n",
       " '圔',\n",
       " '堊',\n",
       " '堨',\n",
       " '堮',\n",
       " '娥',\n",
       " '嬒',\n",
       " '岋',\n",
       " '峉',\n",
       " '峨',\n",
       " '峩',\n",
       " '崿',\n",
       " '嶭',\n",
       " '幄',\n",
       " '悪',\n",
       " '惡',\n",
       " '愕',\n",
       " '戄',\n",
       " '戹',\n",
       " '扼',\n",
       " '捾',\n",
       " '握',\n",
       " '搤',\n",
       " '搹',\n",
       " '擜',\n",
       " '擭',\n",
       " '斡',\n",
       " '枂',\n",
       " '枙',\n",
       " '楃',\n",
       " '櫮',\n",
       " '歹',\n",
       " '歺',\n",
       " '沃',\n",
       " '洝',\n",
       " '涐',\n",
       " '涴',\n",
       " '渥',\n",
       " '湂',\n",
       " '濣',\n",
       " '焥',\n",
       " '珴',\n",
       " '琧',\n",
       " '痷',\n",
       " '瘂',\n",
       " '皒',\n",
       " '睋',\n",
       " '矱',\n",
       " '砐',\n",
       " '砨',\n",
       " '硆',\n",
       " '硪',\n",
       " '磀',\n",
       " '罨',\n",
       " '肟',\n",
       " '胺',\n",
       " '腛',\n",
       " '腭',\n",
       " '臒',\n",
       " '臥',\n",
       " '苊',\n",
       " '莪',\n",
       " '萼',\n",
       " '蕚',\n",
       " '蘁',\n",
       " '蚅',\n",
       " '蚵',\n",
       " '蛾',\n",
       " '蝁',\n",
       " '蠖',\n",
       " '訛',\n",
       " '詻',\n",
       " '誐',\n",
       " '誒',\n",
       " '諤',\n",
       " '譌',\n",
       " '讍',\n",
       " '豟',\n",
       " '貖',\n",
       " '軛',\n",
       " '軶',\n",
       " '輵',\n",
       " '迗',\n",
       " '遏',\n",
       " '遻',\n",
       " '鄂',\n",
       " '鈋',\n",
       " '鈪',\n",
       " '鋈',\n",
       " '鋨',\n",
       " '鍔',\n",
       " '鑩',\n",
       " '閼',\n",
       " '阨',\n",
       " '阸',\n",
       " '隘',\n",
       " '隲',\n",
       " '雘',\n",
       " '韄',\n",
       " '頞',\n",
       " '頟',\n",
       " '額',\n",
       " '顎',\n",
       " '餓',\n",
       " '餩',\n",
       " '餲',\n",
       " '騀',\n",
       " '魤',\n",
       " '鰐',\n",
       " '鱷',\n",
       " '鳽',\n",
       " '鵝',\n",
       " '鵞',\n",
       " '鶚',\n",
       " '齃',\n",
       " '齶',\n",
       " '齷',\n",
       " '齾',\n",
       " '𡅅'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_extract_tone('我')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'佤', '咓', '搲', '攨', '瓦', '瓩', '瓸', '邷', '阿'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_extract_finalConsonant('我')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['哦',\n",
       " '锇',\n",
       " '騀',\n",
       " '莪',\n",
       " '睋',\n",
       " '義',\n",
       " '涐',\n",
       " '峨',\n",
       " '珴',\n",
       " '誐',\n",
       " '鵝',\n",
       " '娥',\n",
       " '俄',\n",
       " '饿',\n",
       " '我',\n",
       " '峩',\n",
       " '鋨',\n",
       " '鵞',\n",
       " '蛾',\n",
       " '硪',\n",
       " '鹅',\n",
       " '皒',\n",
       " '餓']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_similar(ch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename, (ch_dict,_) in confu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'他', '她'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_confusion_chPairs[ch_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iii = '請'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = sound_SIGHAN.loc[iii][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "同音同調                                                    頃\n",
       "同音異調                                    青慶罄卿親擎情蜻傾輕磬鯖氫檠晴頃清\n",
       "近音同調                                     頸景井饉緊僅警寢儘覲憬謹瑾阱璟錦\n",
       "近音異調    竟涇靖粳燼精秦靜筋縉噙沁琴進巾芹莖競親噤津儘覲斤經撳近逕今境驚睛矜勁襟擒更徑觔禽兢侵晶旌欽京...\n",
       "Name: 請, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['臥窩沃斡萵齷倭渥渦握']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    try:\n",
    "        row = sound_SIGHAN.loc[ch_x]\n",
    "        for idx, col in enumerate(row[:-1]):\n",
    "            if type(col)==str and col.find(ch_y)!=-1:\n",
    "                return 4-idx\n",
    "                break\n",
    "        else:\n",
    "            return 0\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'齷', '倭', '萵', '握', '渦', '臥', '沃', '斡', '窩', '渥'}\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [col for col in sound_SIGHAN.loc[iii][:-1] if type(col)==str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.update?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch_x = '請'\n",
    "a = set()\n",
    "for col in sound_SIGHAN.loc[ch_x][:-1]:\n",
    "    if type(col)==str:\n",
    "        a.update(list(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_x = '請'\n",
    "\n",
    "cands = set()\n",
    "\n",
    "# Sound Unihan \n",
    "cands.update(sound_extract_same(ch_x))\n",
    "cands.update(sound_extract_similartConsonant(ch_x))\n",
    "cands.update(sound_extract_tone(ch_x))\n",
    "cands.update(sound_extract_finalConsonant(ch_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confuExtract(ch_x):\n",
    "    cands = set()\n",
    "\n",
    "    # Sound Unihan \n",
    "    cands.update(sound_extract_same(ch_x))\n",
    "    cands.update(sound_extract_similartConsonant(ch_x))\n",
    "    cands.update(sound_extract_tone(ch_x))\n",
    "    cands.update(sound_extract_finalConsonant(ch_x))\n",
    "\n",
    "    # print(len(cands))\n",
    "\n",
    "    # Shape Unihan \n",
    "    cands.update(shape_similar(ch_x))\n",
    "\n",
    "    # print(len(cands))\n",
    "\n",
    "\n",
    "    # Sound SIGHAN\n",
    "    for col in sound_SIGHAN.loc[ch_x][:-1]:\n",
    "        if type(col)==str:\n",
    "            cands.update(list(col))\n",
    "\n",
    "    # print(len(cands))\n",
    "\n",
    "    # confusion pair\n",
    "    cands.update(combine_confusion_chPairs[ch_x])\n",
    "\n",
    "#     print(len(cands))\n",
    "    return (cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_only = dict()\n",
    "for ch in ch_common:\n",
    "    confusion_only[ch] = confuExtract(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5361"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(confusion_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226.68214885282597"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(i) for i in confusion_only.values())/len(confusion_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(file=open('co.pkl','wb'),obj=confusion_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "49\n",
      "128\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "ch_x = '請'\n",
    "\n",
    "cands = set()\n",
    "\n",
    "# Sound Unihan \n",
    "# cands.update(sound_extract_same(ch_x))\n",
    "# cands.update(sound_extract_similartConsonant(ch_x))\n",
    "# cands.update(sound_extract_tone(ch_x))\n",
    "# cands.update(sound_extract_finalConsonant(ch_x))\n",
    "\n",
    "# print(len(cands))\n",
    "\n",
    "# Shape Unihan \n",
    "cands.update(shape_similar(ch_x))\n",
    "\n",
    "# print(len(cands))\n",
    "\n",
    "\n",
    "# Sound SIGHAN\n",
    "for col in sound_SIGHAN.loc[ch_x][:-1]:\n",
    "    if type(col)==str:\n",
    "        cands.update(list(col))\n",
    "\n",
    "# print(len(cands))\n",
    "        \n",
    "# confusion pair\n",
    "cands.update(combine_confusion_chPairs[ch_x])\n",
    "\n",
    "print(len(cands))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section_label = 'char_information'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dicBPMF = defaultdict(list)\n",
    "dicPhone = defaultdict(list)\n",
    "dicCangjie = defaultdict(list)\n",
    "dicCangjie2char = defaultdict(list)\n",
    "extract_unihan(filelist['unihan.csv'])\n",
    "\n",
    "sound_SIGHAN = pd.read_csv(\n",
    "    filelist['Bakeoff2013_CharacterSet_SimilarPronunciation.txt'], \n",
    "    sep='\\t', index_col=0)\n",
    "shape_SIGHAN = pd.read_csv(\n",
    "    filelist['Bakeoff2013_CharacterSet_SimilarShape.txt'], \\\n",
    "    sep=',', index_col=0, names=['cands']).to_dict()['cands']\n",
    "\n",
    "voc = extract_zwtTitle(open(filelist['zwt.titles.txt'], encoding='utf8').readlines())\n",
    "\n",
    "dicRadicalnum, dicRadical = radicalDic(\n",
    "    open(filelist['radical.txt'], 'r', encoding='utf8').readlines())\n",
    "\n",
    "dicFreq = extract_bigUnihan(filelist['unihan_utf8_new.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch_common = sound_SIGHAN.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./ch_5401.txt', 'w',encoding='utf8') as fp:\n",
    "    fp.write('\\n'.join(ch_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Error_corr_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section_label = 'error_corr_pair'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Filename: 4教育部錯別字表.txt\n",
      "4教育部錯別字表.txt\n",
      "Original:490\tPost:470\n",
      "ch_dict:416\tword_dict:470\n",
      "\n",
      "== Filename: 3常見錯別字一覽表.txt\n",
      "3常見錯別字一覽表.txt\n",
      "Original:1364\tPost:1265\n",
      "ch_dict:803\tword_dict:1172\n",
      "\n",
      "== Filename: 1新編常用錯別字門診.txt\n",
      "1新編常用錯別字門診.txt\n",
      "Original:490\tPost:470\n",
      "ch_dict:416\tword_dict:470\n",
      "\n",
      "== Filename: udn_common.xls\n",
      "ch_dict:384\tword_dict:1057\n",
      "\n",
      "== Filename: udn_pairs.csv\n",
      "ch_dict:273\tword_dict:0\n",
      "\n",
      "== Filename: 2東東錯別字.txt\n",
      "2東東錯別字.txt\n",
      "Original:57924\tPost:38353\n",
      "ch_dict:3385\tword_dict:37478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))\n",
    "confusion_pairs = dict()\n",
    "# confusion_pairs[special] = extract_pairs_udn\n",
    "for filename, ch_dict, word_dict in extract_pairs(filelist):\n",
    "    print('ch_dict:{}\\tword_dict:{}\\n'.format(len(ch_dict),len(word_dict)))\n",
    "    confusion_pairs[filename] = (ch_dict,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combineConfusionpair(confusion_pairs):\n",
    "    combine_confusion_chPairs = defaultdict(lambda :set())\n",
    "    combine_confusion_wordPairs = defaultdict(lambda :set())\n",
    "    tmp = defaultdict(lambda :set())\n",
    "\n",
    "    for filename, (ch_dict, word_dict) in confusion_pairs.items():\n",
    "        if 'udn' not in filename:\n",
    "            for ch, cands in ch_dict.items():\n",
    "                combine_confusion_chPairs[ch].update(cands)    \n",
    "            for word, cands in word_dict.items():\n",
    "                tmp[word].add(cands)\n",
    "        else:\n",
    "            for ch, cands in ch_dict.items():\n",
    "                combine_confusion_chPairs[ch].update(set(cand for _, cand in cands))\n",
    "\n",
    "\n",
    "    sort_chpairs = sorted(combine_confusion_chPairs.items(), key=lambda x:-len(x[1]))\n",
    "    max_cands = max(len(cands) for _, cands in sort_chpairs)\n",
    "    min_cands = min(len(cands) for _, cands in sort_chpairs)\n",
    "    avg_cands = sum(len(cands) for _, cands in sort_chpairs)/len(sort_chpairs)\n",
    "    print('=== char pairs ')\n",
    "    print('Max cands in confusion pair = {}'.format(max_cands))\n",
    "    print('Min cands in confusion pair = {}'.format(min_cands))\n",
    "    print('Avg cands in confusion pair = {}'.format(avg_cands))\n",
    "\n",
    "    for ch, cands in tmp.items():\n",
    "        for cand in cands:\n",
    "            combine_confusion_wordPairs[cand].add(ch)\n",
    "\n",
    "    sort_wordpairs = sorted(combine_confusion_wordPairs.items(), key=lambda x:-len(x[1]))\n",
    "    max_cands = max(len(cands) for _, cands in sort_wordpairs)\n",
    "    min_cands = min(len(cands) for _, cands in sort_wordpairs)\n",
    "    avg_cands = sum(len(cands) for _, cands in sort_wordpairs)/len(sort_wordpairs)\n",
    "    print('=== word pairs')\n",
    "    print('Max cands in confusion pair = {}'.format(max_cands))\n",
    "    print('Min cands in confusion pair = {}'.format(min_cands))\n",
    "    print('Avg cands in confusion pair = {}'.format(avg_cands))\n",
    "\n",
    "    return combine_confusion_chPairs, combine_confusion_wordPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== char pairs \n",
      "Max cands in confusion pair = 16\n",
      "Min cands in confusion pair = 1\n",
      "Avg cands in confusion pair = 2.4152295038471743\n",
      "=== word pairs\n",
      "Max cands in confusion pair = 15\n",
      "Min cands in confusion pair = 1\n",
      "Avg cands in confusion pair = 2.0436973007371266\n"
     ]
    }
   ],
   "source": [
    "combine_confusion_chPairs, combine_confusion_wordPairs = combineConfusionpair(confusion_pairs)\n",
    "combine_chPairs_filename = './confusionTable/rule_chPairs.pkl'\n",
    "combine_wordPairs_filename = './confusionTable/rule_wordPairs.pkl'\n",
    "\n",
    "pickle.dump(dict(combine_confusion_chPairs), open(combine_chPairs_filename, 'wb'))\n",
    "pickle.dump(dict(combine_confusion_wordPairs), open(combine_wordPairs_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Error_corr_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Filename: SIGHAN15_CSC_B2_Training_utf8.sgml\n",
      "ch_dict:715\tword_dict:1478\n",
      "\n",
      "== Filename: C1_training.sgml\n",
      "ch_dict:237\tword_dict:369\n",
      "\n",
      "== Filename: Bakeoff2013_SampleSet_WithError_utf8.txt\n",
      "ch_dict:0\tword_dict:0\n",
      "\n",
      "== Filename: SIGHAN15_CSC_A2_Training.sgml\n",
      "ch_dict:521\tword_dict:794\n",
      "\n",
      "== Filename: B1_training_utf8.sgml\n",
      "ch_dict:1165\tword_dict:3608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "section_label = 'error_corr_sentence'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))\n",
    "unwated_file = filelist.pop('big5')\n",
    "\n",
    "confusion_sentences = dict()\n",
    "for filename, ch_dict, word_dict, seq_dict in extract_sentences(filelist):\n",
    "    print('ch_dict:{}\\tword_dict:{}\\n'.format(len(ch_dict),len(word_dict)))\n",
    "    confusion_sentences[filename] = (ch_dict,word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Char_probability (Language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language model /home/kiwi/udn_data/training_confusion/sinica.corpus.seg.char.lm ...\n"
     ]
    }
   ],
   "source": [
    "from model.model import LM\n",
    "\n",
    "filename = dataroot('sinica.corpus.seg.char.lm')[:-1]\n",
    "lm = LM(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * POS (CKIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# section_label = 'pos'\n",
    "# filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pos_dict = defaultdict(set)\n",
    "\n",
    "# idx = 0\n",
    "# with open(filelist['CKIP_Dictionary_UTF8.txt'], 'r', encoding='utf8') as fp:\n",
    "#     # Ignore POS contain '!'\n",
    "#     for line in fp:\n",
    "#         data = line.split()\n",
    "#         idx += 1\n",
    "#         if len(data[0])==1:\n",
    "#             pos_tag = data[2] if data[2].find('!')<0 else data[2][1:]\n",
    "#             pos_dict[data[0]].add(pos_tag)\n",
    "#         else:            \n",
    "#             break\n",
    "        \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ch_x, ch_y = '的','得'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pos_x = pos_dict[ch_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pos_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pos_y = pos_dict[ch_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pos_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "dd = {('Caa','Cab'):'C'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def pos_simplify(pos_tags, level=1):\n",
    "#     pass\n",
    "    \n",
    "    \n",
    "#     output = set()    \n",
    "#     for p in pos_tags:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# same common\n",
    "if pos_y.intersection(pos_x):    \n",
    "    print('y')\n",
    "    \n",
    "# Simplified common \n",
    "elif :\n",
    "\n",
    "# basic common \n",
    "elif :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 取前兩碼，不足則全取\n",
    "2. Daa != Dab\n",
    "3. Nc != Ncd\n",
    "4. Neu, Nes, Nep, Neqa, Neqb 保留\n",
    "5. T 取一碼\n",
    "6. VA != VAC \n",
    "7. VH != VHC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''' pandas.Dataframe\n",
    "df = pd.read_csv(filelist['CKIP_Dictionary_UTF8.txt'], sep='\\t', header=None, names=['title','n1','pos','n2','pronunciation','def'])\n",
    "pos_df = df[df.apply(lambda x:len(x['title'])==1, axis=1)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "section_label = 'fasttext'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GOOD\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(filelist['UDN.doc.char.skipgram.vec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (UDN)- NOT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    with open(filepath, 'r') as fp:\n",
    "        for line in fp:\n",
    "            yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(N, tf, df):\n",
    "    return tf * math.log(N / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section_label = 'tfidf'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compare function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fasttext_compare(ch_x, ch_y):\n",
    "    try:\n",
    "        return fasttext_model.similarity(ch_x, ch_y)\n",
    "    except KeyError:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_compare_SIGHAN(ch_x, ch_y):\n",
    "    '''\n",
    "    return (similar, 同部首同筆畫數)\n",
    "    '''\n",
    "    cands1 = shape_SIGHAN.get(ch_x, [])\n",
    "    try:\n",
    "        cands2 = sound_SIGHAN.loc[ch_x].同部首同筆畫數\n",
    "        if type(cands2)==float:\n",
    "            cands2 = []\n",
    "    except KeyError:\n",
    "        cands2 = []\n",
    "    \n",
    "    out1 = 1 if ch_y in cands1 else 0\n",
    "    out2 = 1 if ch_y in cands2 else 0\n",
    "    \n",
    "    return (out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_compare_SIGHAN(ch_x, ch_y):\n",
    "    '''\n",
    "    4. 同音同調\n",
    "    3. 同音異調\n",
    "    2. 近音同調\n",
    "    1. 近音異調\n",
    "    0 Not Found  \n",
    "    '''\n",
    "    try:\n",
    "        row = sound_SIGHAN.loc[ch_x]\n",
    "        for idx, col in enumerate(row[:-1]):\n",
    "            if type(col)==str and col.find(ch_y)!=-1:\n",
    "                return 4-idx\n",
    "                break\n",
    "        else:\n",
    "            return 0\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_compare_unihan(ch_x,ch_y):\n",
    "    if ch_y in sound_extract_same(ch_x):\n",
    "        return 4\n",
    "    elif ch_y in sound_extract_tone(ch_x):\n",
    "        return 3\n",
    "    elif ch_y in sound_extract_similartConsonant(ch_x, toneKeep=True):\n",
    "        return 2\n",
    "    elif ch_y in sound_extract_similartConsonant(ch_x, toneKeep=False):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def shape_compare_unihan(ch_x,ch_y):\n",
    "    if ch_y in shape_similar(ch_x):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cangjie_compare_unihan(ch_x,ch_y):\n",
    "    '''\n",
    "    Compare the cangjie between two character\n",
    "    applied LCS to check whether the two chars have similar cangjie code \n",
    "    '''\n",
    "    \n",
    "    def lcs(xstr, ystr):\n",
    "        \"\"\"\n",
    "        >>> lcs('thisisatest', 'testing123testing')\n",
    "        'tsitest'\n",
    "        \"\"\"\n",
    "        if not xstr or not ystr:\n",
    "            return \"\"\n",
    "        x, xs, y, ys = xstr[0], xstr[1:], ystr[0], ystr[1:]\n",
    "        if x == y:\n",
    "            return x + lcs(xs, ys)\n",
    "        else:\n",
    "            return max(lcs(xstr, ys), lcs(xs, ystr), key=len)\n",
    "    \n",
    "    cang_x = dicCangjie.get(ch_x,[])\n",
    "    cang_y = dicCangjie.get(ch_y,[])\n",
    "    \n",
    "    if len(cang_x)==0 or len(cang_y)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        cang_x == cang_x[0]\n",
    "        cang_y == cang_y[0]\n",
    "    \n",
    "    if cang_x == cang_y:\n",
    "        return 2\n",
    "    else:\n",
    "        lcs_length = len(lcs(cang_x, cang_y))\n",
    "        if len(cang_x) == 2:\n",
    "            if (lcs_length == 1 and len(cang_y)==2)\\\n",
    "            or (lcs_length == 2 and len(cang_y)==3):\n",
    "                return 1\n",
    "        elif len(cang_x) == 3:\n",
    "            if lcs_length == 2 and len(cang_y)<=4:\n",
    "                return 1\n",
    "        elif len(cang_x) == 4:\n",
    "            if lcs_length == 3 and len(cang_y)>=3:\n",
    "                return 1\n",
    "        elif len(cang_y) == 5:\n",
    "            if lcs_length == 4 and len(cang_y)==4:\n",
    "                return 1\n",
    "    \n",
    "    return 0     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compare between character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHARPORT = 5487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comparison4confusion(ch_chunk):\n",
    "    '''Similarity between two characters\n",
    "    * sound unihan \n",
    "    * shape unihan\n",
    "    * cangjie unihan\n",
    "    * sound SIGHAN\n",
    "    * shape SIGHAN\n",
    "    \n",
    "    * shape SIGHAN2\n",
    "    * frequency bigUnihan\n",
    "    * lm of ch_x\n",
    "    * lm of ch_y\n",
    "    \n",
    "    * fasttext \n",
    "    \n",
    "    * many,many confusion pairs\n",
    "    \n",
    "    Args:\n",
    "        ch_chunk (tuple,list): ch_chunk[0]=ch_x, ch_chunk[1]=ch_y\n",
    "    Return:\n",
    "        score (float): score of similariy \n",
    "        log (list): features of the similairy between two characters    \n",
    "    '''\n",
    "    \n",
    "    ch_x = ch_chunk[0]\n",
    "    ch_y = ch_chunk[1]\n",
    "    \n",
    "    log = list()\n",
    "    score = 0.0\n",
    "    \n",
    "    # 0. 4 3 2 1 0\n",
    "    tmp = sound_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "    \n",
    "    # 1. 1 0 \n",
    "    tmp = shape_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "    \n",
    "    # 2. 2 1 0 \n",
    "    tmp = cangjie_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "\n",
    "    # 3. 4 3 2 1 0\n",
    "    tmp = sound_compare_SIGHAN(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "\n",
    "    # 4. 1 0 / 1 0 \n",
    "    tmp = shape_compare_SIGHAN(ch_x,ch_y)\n",
    "    log.extend(tmp)\n",
    "    score = score + tmp[0] + tmp[1]\n",
    "    \n",
    "    # 5. 4 3 2 1 0\n",
    "    tmp = (5-dicFreq.get(ch_y,5))\n",
    "    log.append(tmp)\n",
    "    score += tmp        \n",
    "    \n",
    "    lm_get = 'http://140.112.91.62:{}/api/{}{}'.format    \n",
    "    \n",
    "    # 7. float \n",
    "    # log probability \n",
    "    tmp = lm.scoring(ch_x)\n",
    "#     tmp = requests.get(lm_get(CHARPORT, 1, ch_x)).json()['score']\n",
    "    log.append(tmp)\n",
    "#     score -= tmp\n",
    "    \n",
    "    # 8. float \n",
    "    # log probability \n",
    "    tmp = lm.scoring(ch_y)\n",
    "#     tmp = requests.get(lm_get(CHARPORT, 1, ch_y)).json()['score']\n",
    "    \n",
    "    log.append(tmp)\n",
    "#     score -= tmp\n",
    "        \n",
    "    # 9. fasttext\n",
    "    # Probability \n",
    "    tmp = fasttext_compare(ch_x, ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp*10\n",
    "        \n",
    "    # Last\n",
    "    evidence = []\n",
    "    tmp = 0\n",
    "    for i in [confusion_pairs.items(), confusion_sentences.items()]:        \n",
    "        for filename, (ch_dict,_) in i:\n",
    "            if ch_y in ch_dict.get(ch_x,[]):\n",
    "                tmp += 1\n",
    "                evidence.append(filename)\n",
    "                score += 1        \n",
    "    log.append(tmp)\n",
    "    \n",
    "    # Last: Existed confusion pairs\n",
    "    log.append(evidence)    \n",
    "    \n",
    "    return (score,log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confusion_pairs['1新編常用錯別字門診.txt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comparison4confusion(('事','事'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# comparison4confusion(('世','氏'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractFeature(outputfilename, process_cnt, test=0):\n",
    "    # If ch_x not in 5000 common character, choose the best as candidates\n",
    "    ch_label = set(dicBPMF.keys()).union(set(sound_SIGHAN.index))\n",
    "    ch_n_label = random.choices(list(ch_label),k=test) if test>0 else ch_label    \n",
    "\n",
    "    bigDict = defaultdict(dict)\n",
    "\n",
    "    start_time = time.clock()\n",
    "    with multiprocessing.Pool(processes=process_cnt) as pool:\n",
    "        for ch_x in ch_n_label:\n",
    "            ch_n_inside = list(ch_n_label)\n",
    "            ch_n_inside.remove(ch_x)\n",
    "\n",
    "            ch_chunk = [(ch_x, ch_y) for ch_y in ch_n_inside]\n",
    "\n",
    "            scores = pool.map(comparison4confusion, ch_chunk)\n",
    "            \n",
    "            if ch_x in ch_common:\n",
    "                for idx,(_,ch_y) in enumerate(ch_chunk):\n",
    "                    if scores[idx][0]>5.0:\n",
    "                        bigDict[ch_x][ch_y] = scores[idx]\n",
    "            else:\n",
    "                best_score_idx = scores.index(max(scores))\n",
    "                ch_y = ch_chunk[best_score_idx][1]\n",
    "                bigDict[ch_x][ch_y] = scores[best_score_idx]\n",
    "\n",
    "    with open(outputfilename, 'wb') as fp:\n",
    "        pickle.dump(bigDict,fp)\n",
    "    \n",
    "    print(time.clock()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rawConfusion(outputfilename, process_cnt):\n",
    "    ch_label = set(dicBPMF.keys()).union(set(sound_SIGHAN.index))\n",
    "    ch_n_label = random.choices(list(ch_label),k=test) if test>0 else ch_label    \n",
    "\n",
    "    bigDict = defaultdict(dict)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_command():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output', required=True)\n",
    "    parser.add_argument('--process', type=int, default=8)\n",
    "    parser.add_argument('--count', type=int, default=0)\n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    args = process_command()    \n",
    "    extractFeature(args.output, args.process, args.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 'adsf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a, end='')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('adfa',end='')\n",
    "print('df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
