{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all confusion set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import xlrd\n",
    "import multiprocessing\n",
    "\n",
    "import random\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big unihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigUnihan_extract(filename):\n",
    "    df = pd.read_csv(filename, sep='|', low_memory=False)\n",
    "    df = df[['char','kFrequency']].set_index('char')\n",
    "    df = df[~pd.isnull(df.kFrequency)]\n",
    "    \n",
    "    return df.to_dict()['kFrequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIGHAN_char_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_compare_SIGHAN(ch_x, ch_y):\n",
    "    '''\n",
    "    return (similar, 同部首同筆畫數)\n",
    "    '''\n",
    "    cands1 = shape_SIGHAN.get(ch_x, [])\n",
    "    try:\n",
    "        cands2 = sound_SIGHAN.loc[ch_x].同部首同筆畫數\n",
    "        if type(cands2)==float:\n",
    "            cands2 = []\n",
    "    except KeyError:\n",
    "        cands2 = []\n",
    "    \n",
    "    out1 = 1 if ch_y in cands1 else 0\n",
    "    out2 = 1 if ch_y in cands2 else 0\n",
    "    \n",
    "    return (out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_compare_SIGHAN(ch_x, ch_y):\n",
    "    '''\n",
    "    4. 同音同調\n",
    "    3. 同音異調\n",
    "    2. 近音同調\n",
    "    1. 近音異調\n",
    "    0 Not Found  \n",
    "    '''\n",
    "    try:\n",
    "        row = sound_SIGHAN.loc[ch_x]\n",
    "        for idx, col in enumerate(row[:-1]):\n",
    "            if type(col)==str and col.find(ch_y)!=-1:\n",
    "                return 4-idx\n",
    "                break\n",
    "        else:\n",
    "            return 0\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unihan.csv (注音跟倉頡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unihan_extract(unihan_filename):\n",
    "    global dicBPMF, dicPhone, dicCangjie, dicCangjie2char\n",
    "    with open(unihan_filename, 'r', encoding='utf8') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in spamreader:\n",
    "            row = [cell for cell in row] # unicode\n",
    "            char, bpmf, cangjie, components, jp, kr, name, pinyin_chs, pinyin_cht, char_strokes_count, radical, radical_name, radical_strokes_count = row\n",
    "            for ph in bpmf.split(): # 發音\n",
    "                dicBPMF[char] += [ph]\n",
    "                dicPhone[ph] += [char]\n",
    "            for cj in cangjie.split(): # 倉頡碼\n",
    "                if u\"難\" in cj: continue\n",
    "                for i in range(0, 3):\n",
    "                    if i == len(cj): continue\n",
    "                    # dicBPMF[char]['cangjie'] += [cj[:i]]\n",
    "                    dicCangjie[char] += [cj[i:]]\n",
    "                    # dicCangjie[cj[:i]] += [char]\n",
    "                    dicCangjie2char[cj[i:]] += [char]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Unihan.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zwt.titles.txt (字典)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zwtTitle_train(lines):\n",
    "    d = defaultdict(lambda: 0)\n",
    "    for word in lines:\n",
    "        d[word.strip()] += 1\n",
    "    #d[word.strip().decode('utf-8')[:2]] += 1\n",
    "    #print word.strip().decode('utf-8')[:2]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## radical.txt (部首)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def radicalDic(lines):\n",
    "    dicRadicalnum = defaultdict(list)\n",
    "    dicRadical = defaultdict(list)\n",
    "    for line in lines:\n",
    "        for char in line[5:].strip().split('|'):\n",
    "            dicRadical[char] += [line[:4]]\n",
    "            dicRadicalnum[line[:4]] += [char]\n",
    "    return dicRadicalnum, dicRadical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_similar(char):\n",
    "    return list(set(ch for rnum in dicRadical[char] for ch in dicRadicalnum[rnum]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tone extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_extract_same(char):\n",
    "    '''\n",
    "    Same neutral and tone\n",
    "    '''\n",
    "    return list(set(ch for ph in dicBPMF[char] for ch in dicPhone[ph]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sound_extract_tone(char):\n",
    "    '''\n",
    "    the char of different tone \n",
    "    '''\n",
    "    output = set()\n",
    "    tones = ['ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    for ph in dicBPMF[char]:\n",
    "        if ph[-1] in tones:\n",
    "            for t in tones:\n",
    "                if t == ph[-1]: continue\n",
    "                output = output.union(dicPhone[ph[:-1]+t])\n",
    "        else:\n",
    "            for t in tones:\n",
    "                output = output.union(dicPhone[ph+t])\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sound_extract_finalConsonant(char, toneKeep=True):\n",
    "    '''\n",
    "    單：ㄚㄛㄜㄝ\n",
    "    複：ㄞㄟㄠㄡ\n",
    "    鼻：ㄢㄣㄤㄥ\n",
    "    捲舌：ㄦ\n",
    "    '''\n",
    "    output = set()\n",
    "    tones = ['','ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    consonants = [\n",
    "        ['ㄚ','ㄛ','ㄜ','ㄝ'],\n",
    "        ['ㄞ','ㄟ','ㄠ','ㄡ'],\n",
    "        ['ㄢ','ㄣ','ㄤ','ㄥ']\n",
    "    ]\n",
    "    \n",
    "    for ph in dicBPMF[char]:\n",
    "        # Tone delete\n",
    "        if ph[-1] in tones:\n",
    "            neutral, tone = ph[:-1], ph[-1]\n",
    "        else:\n",
    "            neutral, tone = ph,''\n",
    "        \n",
    "        # Add relevent consonant\n",
    "        for cons in consonants:\n",
    "            if neutral[-1] in cons:\n",
    "                new_neutrals = set(neutral[:-1] + c for c in cons if c!=neutral[-1])\n",
    "                for n in new_neutrals:\n",
    "                    if toneKeep:\n",
    "                        output = output.union(dicPhone[n+tone])\n",
    "                    else:\n",
    "                        for t in tones:\n",
    "                            output = output.union(dicPhone[n+t])\n",
    "                break\n",
    "                               \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_extract_similartConsonant(char, toneKeep=True):\n",
    "    '''\n",
    "    一次只針對一種，不會並用\n",
    "    Initial\n",
    "    ㄈㄏ\n",
    "    ㄋㄌ\n",
    "    ㄓㄗ\n",
    "    ㄔㄘ\n",
    "    Final:\n",
    "    ㄢㄤ\n",
    "    ㄜㄦ\n",
    "    ㄣㄥ\n",
    "    Intermediate:\n",
    "    ㄧㄩ\n",
    "    '''\n",
    "    new_neutrals = set()\n",
    "    output = set()\n",
    "    tones = ['','ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    initial_pairs = [\n",
    "        ['ㄈ','ㄏ'],\n",
    "        ['ㄋ','ㄌ'],\n",
    "        ['ㄓ','ㄗ'],\n",
    "        ['ㄔ','ㄘ']\n",
    "    ]\n",
    "    final_pairs = [\n",
    "        ['ㄢ','ㄤ'],\n",
    "        ['ㄜ','ㄦ'],\n",
    "        ['ㄣ','ㄥ']\n",
    "    ]\n",
    "    inter_pairs = [['ㄧ','ㄩ']]\n",
    "    \n",
    "    for ph in dicBPMF[char]:        \n",
    "        # Tone delete\n",
    "        if ph[-1] in tones:\n",
    "            neutral, tone = ph[:-1], ph[-1]\n",
    "        else:\n",
    "            neutral, tone = ph, ''\n",
    "            \n",
    "        # Initial-consonant, just pick one \n",
    "        for cons in initial_pairs:\n",
    "            if neutral[0] in cons:\n",
    "                new_neutrals = new_neutrals.union(c + neutral[1:] + tone for c in cons if c!=neutral[0])\n",
    "                break        \n",
    "        \n",
    "                    \n",
    "        # Final-consonant       \n",
    "        for cons in final_pairs:\n",
    "#             print(neutral[-1], cons)\n",
    "            if neutral[-1] in cons:\n",
    "#                 print('i', cons)\n",
    "#                 print(neutral[:-1])\n",
    "#                 print(list(neutral[:-1] + c for c in cons if c!=neutral[-1]))\n",
    "                new_neutrals = new_neutrals.union(neutral[:-1] + c + tone for c in cons if c!=neutral[-1])\n",
    "                break\n",
    "        \n",
    "        # Inter_\n",
    "        for cons in inter_pairs:\n",
    "            for idx, tmp in enumerate(neutral):\n",
    "                if tmp in cons:\n",
    "                    new_neutrals = new_neutrals.union(neutral[:idx] + c + neutral[idx+1:] + tone for c in cons if c!=tmp)\n",
    "                    break\n",
    "    \n",
    "    ######## fIX TOne pRoblEm\n",
    "#     print(new_neutrals)\n",
    "    # Get candidate based on new_neutrals\n",
    "    for n in new_neutrals:\n",
    "        if toneKeep:\n",
    "            output = output.union(dicPhone[n])\n",
    "        else:\n",
    "            tmp = n[:-1] if n[-1] in tones else n            \n",
    "            for t in tones:\n",
    "                output = output.union(dicPhone[n+t])\n",
    "#         print(n,' '.join(output))\n",
    "        \n",
    "#     print(len(output))                     \n",
    "    return output if len(output)>0 else []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cangjie Extraction (from UNIHAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cangjie_extract_same(char):\n",
    "    cang = dicCangjie[char]\n",
    "    if len(cang) > 0:\n",
    "        output = set(dicCangjie2char[cang[0]])\n",
    "        output.remove(char)\n",
    "    else:\n",
    "        output = set()\n",
    "    \n",
    "    return list(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cangjie_compare_unihan(ch_x,ch_y):\n",
    "    '''\n",
    "    Compare the cangjie between two character\n",
    "    applied LCS to check whether the two chars have similar cangjie code \n",
    "    '''\n",
    "    \n",
    "    def lcs(xstr, ystr):\n",
    "        \"\"\"\n",
    "        >>> lcs('thisisatest', 'testing123testing')\n",
    "        'tsitest'\n",
    "        \"\"\"\n",
    "        if not xstr or not ystr:\n",
    "            return \"\"\n",
    "        x, xs, y, ys = xstr[0], xstr[1:], ystr[0], ystr[1:]\n",
    "        if x == y:\n",
    "            return x + lcs(xs, ys)\n",
    "        else:\n",
    "            return max(lcs(xstr, ys), lcs(xs, ystr), key=len)\n",
    "    \n",
    "    cang_x = dicCangjie.get(ch_x,[])\n",
    "    cang_y = dicCangjie.get(ch_y,[])\n",
    "    \n",
    "    if len(cang_x)==0 or len(cang_y)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        cang_x == cang_x[0]\n",
    "        cang_y == cang_y[0]\n",
    "    \n",
    "    if cang_x == cang_y:\n",
    "        return 2\n",
    "    else:\n",
    "        lcs_length = len(lcs(cang_x, cang_y))\n",
    "        if len(cang_x) == 2:\n",
    "            if (lcs_length == 1 and len(cang_y)==2)\\\n",
    "            or (lcs_length == 2 and len(cang_y)==3):\n",
    "                return 1\n",
    "        elif len(cang_x) == 3:\n",
    "            if lcs_length == 2 and len(cang_y)<=4:\n",
    "                return 1\n",
    "        elif len(cang_x) == 4:\n",
    "            if lcs_length == 3 and len(cang_y)>=3:\n",
    "                return 1\n",
    "        elif len(cang_y) == 5:\n",
    "            if lcs_length == 4 and len(cang_y)==4:\n",
    "                return 1\n",
    "    \n",
    "    return 0     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error_correct pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractPairs(filelist):\n",
    "    for filename, path in filelist.items():\n",
    "        print('== Filename: {}'.format(filename))\n",
    "        '''\n",
    "        QQQQQQ 有兩個以上的錯誤在一個詞裡面，但更正只有一項\n",
    "        把上方例子放棄不取\n",
    "        有 duplicate \n",
    "        '''\n",
    "        # 1新編常用錯別字門診.txt OR 4教育部錯別字表.txt\n",
    "        if filename.startswith('1') or filename.startswith('4'):\n",
    "            df = pd.read_csv(path, sep='\\t')\n",
    "        # 2東東錯別字.txt OR 3常見錯別字一覽表.txt\n",
    "        elif filename.startswith('2') or filename.startswith('3'):        \n",
    "            df = pd.read_csv(path, sep='\\t', header=None, names = ['正確詞','錯誤詞','正確字','錯誤字'])\n",
    "        elif filename.startswith('udn_common'):\n",
    "            table = xlrd.open_workbook(path).sheet_by_index(0)\n",
    "            ch_dict = defaultdict(set)\n",
    "            # Have multierros (error_word to correct_word)\n",
    "            word_dict = defaultdict(set)\n",
    "            for idx in range(1,table.nrows):\n",
    "                row = table.row_values(idx)[:5]\n",
    "                # Consider the priority of pairs \n",
    "                if row[2].strip():            \n",
    "                    chs = row[2].split()\n",
    "                    if len(chs)==1:\n",
    "                        continue\n",
    "                    for i in range(1,len(chs)):\n",
    "                        freq = row[1] if type(row[1])==float else 1.0                \n",
    "                        ch_dict[chs[i]].add((int(freq),chs[0]))\n",
    "                elif row[3].strip():\n",
    "                    corr_seq = row[3].strip()\n",
    "                    error_seq = row[4].strip()\n",
    "                    word_dict[error_seq] = corr_seq\n",
    "            yield (filename, ch_dict, word_dict)\n",
    "            continue\n",
    "        elif filename.startswith('udn_pairs'):\n",
    "            ch_dict = defaultdict(set)\n",
    "            with open(path, 'r', encoding='utf8') as fp:\n",
    "                for line in fp:\n",
    "                    tt = line.split()\n",
    "                    if int(tt[2])>10:\n",
    "                        ch_dict[tt[0]].add((int(tt[2]), tt[1]))\n",
    "                    \n",
    "            yield (filename, ch_dict, dict())\n",
    "            continue\n",
    "        \n",
    "        print(filename)\n",
    "        \n",
    "        # For 1,2,3,4\n",
    "        if len(df)>0:\n",
    "            df = df.dropna()\n",
    "            df['idx'] = df.apply(lambda x:x['錯誤詞'].find(x['錯誤字']), axis=1)\n",
    "            df['pair'] = tuple(zip(df['idx'], df['錯誤字']))\n",
    "            df['noMultiErrors'] = df.apply(lambda x:x['正確詞']==x['錯誤詞'].replace(x['錯誤字'],x['正確字']), axis=1)\n",
    "            \n",
    "            # Remove multi-errors for the lack of right answer \n",
    "            preCnt = len(df)\n",
    "            df = df[df['noMultiErrors']==True]\n",
    "            postCnt = len(df)\n",
    "            \n",
    "            print('Original:{}\\tPost:{}'.format(preCnt,postCnt))\n",
    "            \n",
    "            df = df.set_index('錯誤詞')\n",
    "            \n",
    "            # Output DICT{'error_word':'(idx, corr_ch)'}\n",
    "#             df_slice = df[['pair']]\n",
    "#             word_dict = df_slice.to_dict()['pair']\n",
    "            word_dict = df[['正確詞']].to_dict()['正確詞']\n",
    "\n",
    "            # output DICT{'error_ch':set(cands)}\n",
    "            ch_dict = defaultdict(lambda :set())\n",
    "            pairs = tuple(zip(df['錯誤字'], df['正確字']))\n",
    "            for error_ch, corr_ch in pairs:\n",
    "                ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "            yield (filename, ch_dict, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Sentnece (from SIGHAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bakeoff-2013 not work\n",
    "2. sequence error not append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractSentence(filelist):\n",
    "    for filename, path in filelist.items():\n",
    "        print('== Filename: {}'.format(filename))\n",
    "        \n",
    "        with open(path,'r',encoding='utf8') as fp:\n",
    "            soup = BeautifulSoup(fp, 'lxml')\n",
    "        \n",
    "        ch_dict = defaultdict(set)\n",
    "        word_dict = defaultdict(set)\n",
    "        seq_dict  = defaultdict(set)\n",
    "        \n",
    "        # Different label\n",
    "        if filename.startswith('Bakeoff'):\n",
    "            pass \n",
    "            ############## NOT FIX\n",
    "            for idx,element in enumerate(soup.find_all('DOC')):  \n",
    "                # Text\n",
    "                text = dict()\n",
    "                for pas in element.find('p').find_all('passage'):\n",
    "                    text[pas.get('id')] = pas.string\n",
    "\n",
    "                # Mistake\n",
    "                for mistake in element.find_all('mistake'):\n",
    "                    mis_id = mistake.get('id')\n",
    "                    mis_loc = mistake.get('location')\n",
    "                    mis_wrong = mistake.find('wrong').string.strip()\n",
    "                    mis_corr  = mistake.find('correction').string.strip()\n",
    "                    cur_seq = text.get(mis_id, '')\n",
    "\n",
    "                    pairs =  [(mis_wrong,idx,x,y) for idx, (x,y) in enumerate(zip(mis_wrong, mis_corr)) if x!=y]\n",
    "\n",
    "                    # error-corr\n",
    "                    for mis_wrong,idx,error_ch,corr_ch in pairs:\n",
    "                        # char-based\n",
    "                        ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "                        # word-based\n",
    "                        word_dict[mis_wrong].add((idx,corr_ch))\n",
    "\n",
    "        else:            \n",
    "            for idx,element in enumerate(soup.find_all('essay')):  \n",
    "                # Text\n",
    "                text = dict()\n",
    "                for pas in element.find('text').find_all('passage'):\n",
    "                    text[pas.get('id')] = pas.string\n",
    "\n",
    "                # Mistake\n",
    "                for mistake in element.find_all('mistake'):\n",
    "                    mis_id = mistake.get('id')\n",
    "                    mis_loc = mistake.get('location')\n",
    "                    mis_wrong = mistake.find('wrong').string.strip()\n",
    "                    mis_corr  = mistake.find('correction').string.strip()\n",
    "                    cur_seq = text.get(mis_id, '')\n",
    "\n",
    "                    pairs =  [(mis_wrong,idx,x,y) for idx, (x,y) in enumerate(zip(mis_wrong, mis_corr)) if x!=y]\n",
    "\n",
    "                    # error-corr\n",
    "                    for mis_wrong,idx,error_ch,corr_ch in pairs:\n",
    "                        # char-based\n",
    "                        ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "                        # word-based\n",
    "                        word_dict[mis_wrong].add((idx,corr_ch))\n",
    "\n",
    "                        # sequence-based \n",
    "                        ### Have problem with multiple errors in single word \n",
    "            #             seq_dict[cur_seq].add((int(mis_loc)-1,corr_ch))\n",
    "    \n",
    "        yield (filename, ch_dict, word_dict, seq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataroot = 'G:/UDN/training_confusion/{}/'.format\n",
    "# dataroot = '/home/kiwi/udn_data/training_confusion/{}/'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Char information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section_label = 'char_information'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dicBPMF = defaultdict(list)\n",
    "dicPhone = defaultdict(list)\n",
    "dicCangjie = defaultdict(list)\n",
    "dicCangjie2char = defaultdict(list)\n",
    "unihan_extract(filelist['unihan.csv'])\n",
    "\n",
    "sound_SIGHAN = pd.read_csv(\n",
    "    filelist['Bakeoff2013_CharacterSet_SimilarPronunciation.txt'], \n",
    "    sep='\\t', index_col=0)\n",
    "shape_SIGHAN = pd.read_csv(\n",
    "    filelist['Bakeoff2013_CharacterSet_SimilarShape.txt'], \\\n",
    "    sep=',', index_col=0, names=['cands']).to_dict()['cands']\n",
    "\n",
    "voc = zwtTitle_train(open(filelist['zwt.titles.txt'], encoding='utf8').readlines())\n",
    "\n",
    "dicRadicalnum, dicRadical = radicalDic(\n",
    "    open(filelist['radical.txt'], 'r', encoding='utf8').readlines())\n",
    "\n",
    "dicFreq = bigUnihan_extract(filelist['unihan_utf8_new.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Error_corr_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section_label = 'error_corr_pair'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Filename: 1新編常用錯別字門診.txt\n",
      "1新編常用錯別字門診.txt\n",
      "Original:490\tPost:470\n",
      "ch_dict:416\tword_dict:470\n",
      "\n",
      "== Filename: 3常見錯別字一覽表.txt\n",
      "3常見錯別字一覽表.txt\n",
      "Original:1364\tPost:1265\n",
      "ch_dict:803\tword_dict:1172\n",
      "\n",
      "== Filename: 4教育部錯別字表.txt\n",
      "4教育部錯別字表.txt\n",
      "Original:490\tPost:470\n",
      "ch_dict:416\tword_dict:470\n",
      "\n",
      "== Filename: 2東東錯別字.txt\n",
      "2東東錯別字.txt\n",
      "Original:57924\tPost:38353\n",
      "ch_dict:3385\tword_dict:37478\n",
      "\n",
      "== Filename: udn_common.xls\n",
      "ch_dict:384\tword_dict:1057\n",
      "\n",
      "== Filename: udn_pairs.csv\n",
      "ch_dict:273\tword_dict:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))\n",
    "confusion_pairs = dict()\n",
    "# confusion_pairs[special] = extractPairs_udn\n",
    "for filename, ch_dict, word_dict in extractPairs(filelist):\n",
    "    print('ch_dict:{}\\tword_dict:{}\\n'.format(len(ch_dict),len(word_dict)))\n",
    "    confusion_pairs[filename] = (ch_dict,word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Error_corr_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Filename: SIGHAN15_CSC_B2_Training_utf8.sgml\n",
      "ch_dict:715\tword_dict:1478\n",
      "\n",
      "== Filename: Bakeoff2013_SampleSet_WithError_utf8.txt\n",
      "ch_dict:0\tword_dict:0\n",
      "\n",
      "== Filename: SIGHAN15_CSC_A2_Training.sgml\n",
      "ch_dict:521\tword_dict:794\n",
      "\n",
      "== Filename: C1_training.sgml\n",
      "ch_dict:237\tword_dict:369\n",
      "\n",
      "== Filename: B1_training_utf8.sgml\n",
      "ch_dict:1165\tword_dict:3608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "section_label = 'error_corr_sentence'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))\n",
    "unwated_file = filelist.pop('big5')\n",
    "\n",
    "confusion_sentences = dict()\n",
    "for filename, ch_dict, word_dict, seq_dict in extractSentence(filelist):\n",
    "    print('ch_dict:{}\\tword_dict:{}\\n'.format(len(ch_dict),len(word_dict)))\n",
    "    confusion_sentences[filename] = (ch_dict,word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Char_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language model G:/UDN/training_confusion/sinica.corpus.seg.char.lm ...\n"
     ]
    }
   ],
   "source": [
    "from model.model import LM\n",
    "\n",
    "filename = dataroot('sinica.corpus.seg.char.lm')\n",
    "lm = LM(filename[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.65973"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.scoring('地')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * POS (CKIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion_training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_compare_unihan(ch_x,ch_y):\n",
    "    if ch_y in sound_extract_same(ch_x):\n",
    "        return 4\n",
    "    elif ch_y in sound_extract_tone(ch_x):\n",
    "        return 3\n",
    "    elif ch_y in sound_extract_similartConsonant(ch_x, toneKeep=True):\n",
    "        return 2\n",
    "    elif ch_y in sound_extract_similartConsonant(ch_x, toneKeep=False):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def shape_compare_unihan(ch_x,ch_y):\n",
    "    if ch_y in shape_similar(ch_x):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comparison4confusion(ch_chunk):\n",
    "    '''Similarity between two characters\n",
    "    Args:\n",
    "        ch_chunk (tuple,list): ch_chunk[0]=ch_x, ch_chunk[1]=ch_y\n",
    "    Return:\n",
    "        score (float): score of similariy \n",
    "        log (list): features of the similairy between two characters    \n",
    "    '''\n",
    "    \n",
    "    ch_x = ch_chunk[0]\n",
    "    ch_y = ch_chunk[1]\n",
    "    \n",
    "    log = list()\n",
    "    score = 0.0\n",
    "    \n",
    "    # 0. 4 3 2 1 0\n",
    "    tmp = sound_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "    \n",
    "    # 1. 1 0 \n",
    "    tmp = shape_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "    \n",
    "    # 2. 2 1 0 \n",
    "    tmp = cangjie_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "\n",
    "    # 3. 4 3 2 1 0\n",
    "    tmp = sound_compare_SIGHAN(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "\n",
    "    # 4. 1 0 / 1 0 \n",
    "    tmp = shape_compare_SIGHAN(ch_x,ch_y)\n",
    "    log.extend(tmp)\n",
    "    score = score + tmp[0] + tmp[1]\n",
    "    \n",
    "    # 5. 4 3 2 1 0\n",
    "    tmp = (5.0-dicFreq.get(ch_y,5))\n",
    "    log.append(tmp)\n",
    "    score += tmp        \n",
    "    \n",
    "    # 6. \n",
    "    evidence = []\n",
    "    for i in [confusion_pairs.items(), confusion_sentences.items()]:\n",
    "        tmp = 0\n",
    "        for filename, (ch_dict,_) in i:\n",
    "            if ch_y in ch_dict.get(ch_x,[]):\n",
    "                tmp += 1\n",
    "                evidence.append(filename)\n",
    "                score += 1\n",
    "        \n",
    "        log.append(tmp)\n",
    "    \n",
    "    # 7. float \n",
    "    # log probability \n",
    "    tmp = lm.scoring(ch_x)\n",
    "    log.append(tmp)\n",
    "    score -= tmp\n",
    "    \n",
    "    # 8. float \n",
    "    # log probability \n",
    "    tmp = lm.scoring(ch_y)\n",
    "    log.append(tmp)\n",
    "    score -= tmp\n",
    "        \n",
    "    # \n",
    "    \n",
    "    # Last: Existed confusion pairs\n",
    "    log.append(evidence)    \n",
    "    \n",
    "    return (score,log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'世', '是'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_pairs['1新編常用錯別字門診.txt'][0]['事']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101.947132, [0, 0, 0, 0, 0, 0, 0.0, 0, 0, -2.947132, -99.0, []])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison4confusion(('事','朩'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comparison(ch_x, ch_y):\n",
    "    print('=== Comparison')\n",
    "    print(ch_x, ch_y)\n",
    "    '''\n",
    "    相似音的處理\n",
    "    sound_extract_same\n",
    "    sound_extract_tone\n",
    "    sound_extract_similartConsonant\n",
    "    sound_extract_finalConsonant\n",
    "\n",
    "    同音同調 同音異調 異音同調 異音異調\n",
    "    '''\n",
    "        \n",
    "        \n",
    "    print('\\n=== Sound similar (from unihan)')\n",
    "    print(sound_compare_unihan(ch_x,ch_y))\n",
    "    \n",
    "    '''\n",
    "    shape_similar\n",
    "    '''\n",
    "    print('\\n=== Shape similar (from radical)')\n",
    "    print(shape_compare_unihan(ch_x,ch_y))\n",
    "\n",
    "    '''\n",
    "    cangjie_compare\n",
    "    2 same cangjie code \n",
    "    1 similar cangjie code \n",
    "    0 nothing special\n",
    "    '''\n",
    "    print('\\n=== Cangjie (from unihan)')\n",
    "    print( cangjie_compare_unihan(ch_x,ch_y))\n",
    "        \n",
    "    '''\n",
    "    SIGHAN\n",
    "    '''\n",
    "    print('\\n=== SIGHAN Data (sound)')\n",
    "    print(sound_compare_SIGHAN(ch_x, ch_y))\n",
    "    \n",
    "\n",
    "    print('\\n=== SIGHAN Data (shape)')\n",
    "    print(shape_compare_SIGHAN(ch_x,ch_y))\n",
    "#     if t[0]==1:\n",
    "#         print('Similar shape')\n",
    "#     if t[1]==1:\n",
    "#         print('同部首同筆畫數')\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Error-correct pair \n",
    "    'filename': (ch_dict,word_dict)\n",
    "    '''\n",
    "    print('\\n=== Error-correct pair')\n",
    "    for filename, (ch_dict,_) in confusion_pairs.items():\n",
    "        if ch_y in ch_dict.get(ch_x,[]):\n",
    "            print(filename)\n",
    "\n",
    "    '''\n",
    "    Error-correct sentence \n",
    "    'filename': (ch_dict, word_dict)\n",
    "    '''\n",
    "    print('\\n=== Error-correct sentence')\n",
    "    for filename, (ch_dict,_) in confusion_sentences.items():\n",
    "        if ch_y in ch_dict.get(ch_x,[]):\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ch_x = '相'\n",
    "\n",
    "# ch_y = random.choice(list(rr))\n",
    "\n",
    "# comparison(ch_x,ch_y)\n",
    "\n",
    "# ch_x = random.choice(sound_SIGHAN.index)\n",
    "# ch_y = random.choice(sound_SIGHAN.index)\n",
    "\n",
    "# comparison(ch_x,ch_y)\n",
    "\n",
    "if ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractFeature(outputfilename, process_cnt, test=0):\n",
    "    if test == 0:\n",
    "        ch_n_label = set(dicBPMF.keys()).union(set(sound_SIGHAN.index))\n",
    "    else:        \n",
    "        ch_n_label = random.choices(list(ch_label),k=test)\n",
    "\n",
    "#     %%timeit -n 1 -r 1\n",
    "\n",
    "    # pool_size=multiprocessing.cpu_count()\n",
    "\n",
    "    bigDict = defaultdict(dict)\n",
    "\n",
    "    start_time = time.clock()\n",
    "    with multiprocessing.Pool(processes=process_cnt) as pool:\n",
    "        for ch_x in ch_n_label:\n",
    "            ch_n_inside = list(ch_n_label)\n",
    "\n",
    "            ch_n_inside.remove(ch_x)\n",
    "\n",
    "            ch_chunk = [(ch_x, ch_y) for ch_y in ch_n_inside]\n",
    "\n",
    "            scores = pool.map(comparison4confusion, ch_chunk)\n",
    "\n",
    "            for idx,(_,ch_y) in enumerate(ch_chunk):\n",
    "                if scores[idx][0]>0.0:\n",
    "    #             if scores[idx] >= 0:\n",
    "                    bigDict[ch_x][ch_y] = scores[idx]\n",
    "            bigDict[ch_x][ch_x] = (30,[])\n",
    "\n",
    "    with open(outputfilename, 'wb') as fp:\n",
    "        pickle.dump(bigDict,fp)\n",
    "    \n",
    "    print(time.clock()-start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    if len(sys.argv) == 3:\n",
    "        extractFeature(\n",
    "            outputfilename=sys.argv[1], \n",
    "            process_cnt=sys.argv[2], \n",
    "            test=5)\n",
    "    else:\n",
    "        sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
