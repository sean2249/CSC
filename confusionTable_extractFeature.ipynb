{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pickle\n",
    "# import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import xlrd\n",
    "import multiprocessing\n",
    "\n",
    "import random\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Extract function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigunihan_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_bigUnihan(filename):\n",
    "    df = pd.read_csv(filename, sep='|', low_memory=False)\n",
    "    df = df[['char','kFrequency']].set_index('char')\n",
    "    df = df[~pd.isnull(df.kFrequency)]\n",
    "    \n",
    "    return df.to_dict()['kFrequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unihan.csv (注音跟倉頡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_unihan(unihan_filename):\n",
    "    global dicBPMF, dicPhone, dicCangjie, dicCangjie2char\n",
    "    with open(unihan_filename, 'r', encoding='utf8') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in spamreader:\n",
    "            row = [cell for cell in row] # unicode\n",
    "            char, bpmf, cangjie, components, jp, kr, name, pinyin_chs, pinyin_cht, char_strokes_count, radical, radical_name, radical_strokes_count = row\n",
    "            for ph in bpmf.split(): # 發音\n",
    "                dicBPMF[char] += [ph]\n",
    "                dicPhone[ph] += [char]\n",
    "            for cj in cangjie.split(): # 倉頡碼\n",
    "                if u\"難\" in cj: continue\n",
    "                for i in range(0, 3):\n",
    "                    if i == len(cj): continue\n",
    "                    # dicBPMF[char]['cangjie'] += [cj[:i]]\n",
    "                    dicCangjie[char] += [cj[i:]]\n",
    "                    # dicCangjie[cj[:i]] += [char]\n",
    "                    dicCangjie2char[cj[i:]] += [char]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unihan: sound similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_extract_same(char):\n",
    "    '''\n",
    "    Same neutral and tone\n",
    "    '''\n",
    "    return list(set(ch for ph in dicBPMF[char] for ch in dicPhone[ph]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sound_extract_tone(char):\n",
    "    '''\n",
    "    the char of different tone \n",
    "    '''\n",
    "    output = set()\n",
    "    tones = ['ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    for ph in dicBPMF[char]:\n",
    "        if ph[-1] in tones:\n",
    "            for t in tones:\n",
    "                if t == ph[-1]: continue\n",
    "                output = output.union(dicPhone[ph[:-1]+t])\n",
    "        else:\n",
    "            for t in tones:\n",
    "                output = output.union(dicPhone[ph+t])\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sound_extract_finalConsonant(char, toneKeep=True):\n",
    "    '''\n",
    "    單：ㄚㄛㄜㄝ\n",
    "    複：ㄞㄟㄠㄡ\n",
    "    鼻：ㄢㄣㄤㄥ\n",
    "    捲舌：ㄦ\n",
    "    '''\n",
    "    output = set()\n",
    "    tones = ['','ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    consonants = [\n",
    "        ['ㄚ','ㄛ','ㄜ','ㄝ'],\n",
    "        ['ㄞ','ㄟ','ㄠ','ㄡ'],\n",
    "        ['ㄢ','ㄣ','ㄤ','ㄥ']\n",
    "    ]\n",
    "    \n",
    "    for ph in dicBPMF[char]:\n",
    "        # Tone delete\n",
    "        if ph[-1] in tones:\n",
    "            neutral, tone = ph[:-1], ph[-1]\n",
    "        else:\n",
    "            neutral, tone = ph,''\n",
    "        \n",
    "        # Add relevent consonant\n",
    "        for cons in consonants:\n",
    "            if neutral[-1] in cons:\n",
    "                new_neutrals = set(neutral[:-1] + c for c in cons if c!=neutral[-1])\n",
    "                for n in new_neutrals:\n",
    "                    if toneKeep:\n",
    "                        output = output.union(dicPhone[n+tone])\n",
    "                    else:\n",
    "                        for t in tones:\n",
    "                            output = output.union(dicPhone[n+t])\n",
    "                break\n",
    "                               \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_extract_similartConsonant(char, toneKeep=True):\n",
    "    '''\n",
    "    一次只針對一種，不會並用\n",
    "    Initial\n",
    "    ㄈㄏ\n",
    "    ㄋㄌ\n",
    "    ㄓㄗ\n",
    "    ㄔㄘ\n",
    "    Final:\n",
    "    ㄢㄤ\n",
    "    ㄜㄦ\n",
    "    ㄣㄥ\n",
    "    Intermediate:\n",
    "    ㄧㄩ\n",
    "    '''\n",
    "    new_neutrals = set()\n",
    "    output = set()\n",
    "    tones = ['','ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    initial_pairs = [\n",
    "        ['ㄈ','ㄏ'],\n",
    "        ['ㄋ','ㄌ'],\n",
    "        ['ㄓ','ㄗ'],\n",
    "        ['ㄔ','ㄘ']\n",
    "    ]\n",
    "    final_pairs = [\n",
    "        ['ㄢ','ㄤ'],\n",
    "        ['ㄜ','ㄦ'],\n",
    "        ['ㄣ','ㄥ']\n",
    "    ]\n",
    "    inter_pairs = [['ㄧ','ㄩ']]\n",
    "    \n",
    "    for ph in dicBPMF[char]:        \n",
    "        # Tone delete\n",
    "        if ph[-1] in tones:\n",
    "            neutral, tone = ph[:-1], ph[-1]\n",
    "        else:\n",
    "            neutral, tone = ph, ''\n",
    "            \n",
    "        # Initial-consonant, just pick one \n",
    "        for cons in initial_pairs:\n",
    "            if neutral[0] in cons:\n",
    "                new_neutrals = new_neutrals.union(c + neutral[1:] + tone for c in cons if c!=neutral[0])\n",
    "                break        \n",
    "        \n",
    "                    \n",
    "        # Final-consonant       \n",
    "        for cons in final_pairs:\n",
    "#             print(neutral[-1], cons)\n",
    "            if neutral[-1] in cons:\n",
    "#                 print('i', cons)\n",
    "#                 print(neutral[:-1])\n",
    "#                 print(list(neutral[:-1] + c for c in cons if c!=neutral[-1]))\n",
    "                new_neutrals = new_neutrals.union(neutral[:-1] + c + tone for c in cons if c!=neutral[-1])\n",
    "                break\n",
    "        \n",
    "        # Inter_\n",
    "        for cons in inter_pairs:\n",
    "            for idx, tmp in enumerate(neutral):\n",
    "                if tmp in cons:\n",
    "                    new_neutrals = new_neutrals.union(neutral[:idx] + c + neutral[idx+1:] + tone for c in cons if c!=tmp)\n",
    "                    break\n",
    "    \n",
    "    ######## fIX TOne pRoblEm\n",
    "#     print(new_neutrals)\n",
    "    # Get candidate based on new_neutrals\n",
    "    for n in new_neutrals:\n",
    "        if toneKeep:\n",
    "            output = output.union(dicPhone[n])\n",
    "        else:\n",
    "            tmp = n[:-1] if n[-1] in tones else n            \n",
    "            for t in tones:\n",
    "                output = output.union(dicPhone[n+t])\n",
    "#         print(n,' '.join(output))\n",
    "        \n",
    "#     print(len(output))                     \n",
    "    return output if len(output)>0 else []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unihan: same cangjie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# No use\n",
    "def cangjie_extract_same(char):\n",
    "    cang = dicCangjie[char]\n",
    "    if len(cang) > 0:\n",
    "        output = set(dicCangjie2char[cang[0]])\n",
    "        output.remove(char)\n",
    "    else:\n",
    "        output = set()\n",
    "    \n",
    "    return list(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zwt.titles.txt (字典)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_zwtTitle(lines):\n",
    "    d = defaultdict(lambda: 0)\n",
    "    for word in lines:\n",
    "        d[word.strip()] += 1\n",
    "    #d[word.strip().decode('utf-8')[:2]] += 1\n",
    "    #print word.strip().decode('utf-8')[:2]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## radical.txt (部首)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def radicalDic(lines):\n",
    "    dicRadicalnum = defaultdict(list)\n",
    "    dicRadical = defaultdict(list)\n",
    "    for line in lines:\n",
    "        for char in line[5:].strip().split('|'):\n",
    "            dicRadical[char] += [line[:4]]\n",
    "            dicRadicalnum[line[:4]] += [char]\n",
    "    return dicRadicalnum, dicRadical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_similar(char):\n",
    "    return list(set(ch for rnum in dicRadical[char] for ch in dicRadicalnum[rnum]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error_correct pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_pairs(filelist):\n",
    "    for filename, path in filelist.items():\n",
    "        print('== Filename: {}'.format(filename))\n",
    "        '''\n",
    "        QQQQQQ 有兩個以上的錯誤在一個詞裡面，但更正只有一項\n",
    "        把上方例子放棄不取\n",
    "        有 duplicate \n",
    "        '''\n",
    "        # 1新編常用錯別字門診.txt OR 4教育部錯別字表.txt\n",
    "        if filename.startswith('1') or filename.startswith('4'):\n",
    "            df = pd.read_csv(path, sep='\\t')\n",
    "        # 2東東錯別字.txt OR 3常見錯別字一覽表.txt\n",
    "        elif filename.startswith('2') or filename.startswith('3'):        \n",
    "            df = pd.read_csv(path, sep='\\t', header=None, names = ['正確詞','錯誤詞','正確字','錯誤字'])\n",
    "        elif filename.startswith('udn_common'):\n",
    "            table = xlrd.open_workbook(path).sheet_by_index(0)\n",
    "            ch_dict = defaultdict(set)\n",
    "            # Have multierros (error_word to correct_word)\n",
    "            word_dict = defaultdict(set)\n",
    "            for idx in range(1,table.nrows):\n",
    "                row = table.row_values(idx)[:5]\n",
    "                # Consider the priority of pairs \n",
    "                if row[2].strip():            \n",
    "                    chs = row[2].split()\n",
    "                    if len(chs)==1:\n",
    "                        continue\n",
    "                    for i in range(1,len(chs)):\n",
    "                        freq = row[1] if type(row[1])==float else 1.0                \n",
    "                        ch_dict[chs[i]].add((int(freq),chs[0]))\n",
    "                elif row[3].strip():\n",
    "                    corr_seq = row[3].strip()\n",
    "                    error_seq = row[4].strip()\n",
    "                    word_dict[error_seq] = corr_seq\n",
    "            yield (filename, ch_dict, word_dict)\n",
    "            continue\n",
    "        elif filename.startswith('udn_pairs'):\n",
    "            ch_dict = defaultdict(set)\n",
    "            with open(path, 'r', encoding='utf8') as fp:\n",
    "                for line in fp:\n",
    "                    tt = line.split()\n",
    "                    if int(tt[2])>10:\n",
    "                        ch_dict[tt[0]].add((int(tt[2]), tt[1]))\n",
    "                    \n",
    "            yield (filename, ch_dict, dict())\n",
    "            continue\n",
    "        \n",
    "        print(filename)\n",
    "        \n",
    "        # For 1,2,3,4\n",
    "        if len(df)>0:\n",
    "            df = df.dropna()\n",
    "            df['idx'] = df.apply(lambda x:x['錯誤詞'].find(x['錯誤字']), axis=1)\n",
    "            df['pair'] = tuple(zip(df['idx'], df['錯誤字']))\n",
    "            df['noMultiErrors'] = df.apply(lambda x:x['正確詞']==x['錯誤詞'].replace(x['錯誤字'],x['正確字']), axis=1)\n",
    "            \n",
    "            # Remove multi-errors for the lack of right answer \n",
    "            preCnt = len(df)\n",
    "            df = df[df['noMultiErrors']==True]\n",
    "            postCnt = len(df)\n",
    "            \n",
    "            print('Original:{}\\tPost:{}'.format(preCnt,postCnt))\n",
    "            \n",
    "            df = df.set_index('錯誤詞')\n",
    "            \n",
    "            # Output DICT{'error_word':'(idx, corr_ch)'}\n",
    "#             df_slice = df[['pair']]\n",
    "#             word_dict = df_slice.to_dict()['pair']\n",
    "            word_dict = df[['正確詞']].to_dict()['正確詞']\n",
    "\n",
    "            # output DICT{'error_ch':set(cands)}\n",
    "            ch_dict = defaultdict(lambda :set())\n",
    "            pairs = tuple(zip(df['錯誤字'], df['正確字']))\n",
    "            for error_ch, corr_ch in pairs:\n",
    "                ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "            yield (filename, ch_dict, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Sentnece \n",
    "\n",
    "1. Bakeoff-2013 not work\n",
    "2. sequence error not append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_sentences(filelist):\n",
    "    for filename, path in filelist.items():\n",
    "        print('== Filename: {}'.format(filename))\n",
    "        \n",
    "        with open(path,'r',encoding='utf8') as fp:\n",
    "            soup = BeautifulSoup(fp, 'lxml')\n",
    "        \n",
    "        ch_dict = defaultdict(set)\n",
    "        word_dict = defaultdict(set)\n",
    "        seq_dict  = defaultdict(set)\n",
    "        \n",
    "        # Different label\n",
    "        if filename.startswith('Bakeoff'):\n",
    "            pass \n",
    "            ############## NOT FIX\n",
    "            for idx,element in enumerate(soup.find_all('DOC')):  \n",
    "                # Text\n",
    "                text = dict()\n",
    "                for pas in element.find('p').find_all('passage'):\n",
    "                    text[pas.get('id')] = pas.string\n",
    "\n",
    "                # Mistake\n",
    "                for mistake in element.find_all('mistake'):\n",
    "                    mis_id = mistake.get('id')\n",
    "                    mis_loc = mistake.get('location')\n",
    "                    mis_wrong = mistake.find('wrong').string.strip()\n",
    "                    mis_corr  = mistake.find('correction').string.strip()\n",
    "                    cur_seq = text.get(mis_id, '')\n",
    "\n",
    "                    pairs =  [(mis_wrong,idx,x,y) for idx, (x,y) in enumerate(zip(mis_wrong, mis_corr)) if x!=y]\n",
    "\n",
    "                    # error-corr\n",
    "                    for mis_wrong,idx,error_ch,corr_ch in pairs:\n",
    "                        # char-based\n",
    "                        ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "                        # word-based\n",
    "                        word_dict[mis_wrong].add((idx,corr_ch))\n",
    "\n",
    "        else:            \n",
    "            for idx,element in enumerate(soup.find_all('essay')):  \n",
    "                # Text\n",
    "                text = dict()\n",
    "                for pas in element.find('text').find_all('passage'):\n",
    "                    text[pas.get('id')] = pas.string\n",
    "\n",
    "                # Mistake\n",
    "                for mistake in element.find_all('mistake'):\n",
    "                    mis_id = mistake.get('id')\n",
    "                    mis_loc = mistake.get('location')\n",
    "                    mis_wrong = mistake.find('wrong').string.strip()\n",
    "                    mis_corr  = mistake.find('correction').string.strip()\n",
    "                    cur_seq = text.get(mis_id, '')\n",
    "\n",
    "                    pairs =  [(mis_wrong,idx,x,y) for idx, (x,y) in enumerate(zip(mis_wrong, mis_corr)) if x!=y]\n",
    "\n",
    "                    # error-corr\n",
    "                    for mis_wrong,idx,error_ch,corr_ch in pairs:\n",
    "                        # char-based\n",
    "                        ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "                        # word-based\n",
    "                        word_dict[mis_wrong].add((idx,corr_ch))\n",
    "\n",
    "                        # sequence-based \n",
    "                        ### Have problem with multiple errors in single word \n",
    "            #             seq_dict[cur_seq].add((int(mis_loc)-1,corr_ch))\n",
    "    \n",
    "        yield (filename, ch_dict, word_dict, seq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataroot = 'G:/UDN/training_confusion/{}/'.format\n",
    "# dataroot = '/home/kiwi/udn_data/training_confusion/{}/'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Char information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section_label = 'char_information'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dicBPMF = defaultdict(list)\n",
    "dicPhone = defaultdict(list)\n",
    "dicCangjie = defaultdict(list)\n",
    "dicCangjie2char = defaultdict(list)\n",
    "extract_unihan(filelist['unihan.csv'])\n",
    "\n",
    "sound_SIGHAN = pd.read_csv(\n",
    "    filelist['Bakeoff2013_CharacterSet_SimilarPronunciation.txt'], \n",
    "    sep='\\t', index_col=0)\n",
    "shape_SIGHAN = pd.read_csv(\n",
    "    filelist['Bakeoff2013_CharacterSet_SimilarShape.txt'], \\\n",
    "    sep=',', index_col=0, names=['cands']).to_dict()['cands']\n",
    "\n",
    "voc = extract_zwtTitle(open(filelist['zwt.titles.txt'], encoding='utf8').readlines())\n",
    "\n",
    "dicRadicalnum, dicRadical = radicalDic(\n",
    "    open(filelist['radical.txt'], 'r', encoding='utf8').readlines())\n",
    "\n",
    "dicFreq = extract_bigUnihan(filelist['unihan_utf8_new.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Error_corr_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section_label = 'error_corr_pair'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Filename: udn_common.xls\n",
      "ch_dict:384\tword_dict:1057\n",
      "\n",
      "== Filename: 3常見錯別字一覽表.txt\n",
      "3常見錯別字一覽表.txt\n",
      "Original:1364\tPost:1265\n",
      "ch_dict:803\tword_dict:1172\n",
      "\n",
      "== Filename: 2東東錯別字.txt\n",
      "2東東錯別字.txt\n",
      "Original:57924\tPost:38353\n",
      "ch_dict:3385\tword_dict:37478\n",
      "\n",
      "== Filename: 4教育部錯別字表.txt\n",
      "4教育部錯別字表.txt\n",
      "Original:490\tPost:470\n",
      "ch_dict:416\tword_dict:470\n",
      "\n",
      "== Filename: 1新編常用錯別字門診.txt\n",
      "1新編常用錯別字門診.txt\n",
      "Original:490\tPost:470\n",
      "ch_dict:416\tword_dict:470\n",
      "\n",
      "== Filename: udn_pairs.csv\n",
      "ch_dict:273\tword_dict:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))\n",
    "confusion_pairs = dict()\n",
    "# confusion_pairs[special] = extract_pairs_udn\n",
    "for filename, ch_dict, word_dict in extract_pairs(filelist):\n",
    "    print('ch_dict:{}\\tword_dict:{}\\n'.format(len(ch_dict),len(word_dict)))\n",
    "    confusion_pairs[filename] = (ch_dict,word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Error_corr_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Filename: C1_training.sgml\n",
      "ch_dict:237\tword_dict:369\n",
      "\n",
      "== Filename: SIGHAN15_CSC_B2_Training_utf8.sgml\n",
      "ch_dict:715\tword_dict:1478\n",
      "\n",
      "== Filename: SIGHAN15_CSC_A2_Training.sgml\n",
      "ch_dict:521\tword_dict:794\n",
      "\n",
      "== Filename: Bakeoff2013_SampleSet_WithError_utf8.txt\n",
      "ch_dict:0\tword_dict:0\n",
      "\n",
      "== Filename: B1_training_utf8.sgml\n",
      "ch_dict:1165\tword_dict:3608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "section_label = 'error_corr_sentence'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))\n",
    "unwated_file = filelist.pop('big5')\n",
    "\n",
    "confusion_sentences = dict()\n",
    "for filename, ch_dict, word_dict, seq_dict in extract_sentences(filelist):\n",
    "    print('ch_dict:{}\\tword_dict:{}\\n'.format(len(ch_dict),len(word_dict)))\n",
    "    confusion_sentences[filename] = (ch_dict,word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Char_probability (Language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language model G:/UDN/training_confusion/sinica.corpus.seg.char.lm ...\n"
     ]
    }
   ],
   "source": [
    "from model.model import LM\n",
    "\n",
    "filename = dataroot('sinica.corpus.seg.char.lm')[:-1]\n",
    "lm = LM(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * POS (CKIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section_label = 'pos'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_dict = defaultdict(set)\n",
    "\n",
    "idx = 0\n",
    "with open(filelist['CKIP_Dictionary_UTF8.txt'], 'r', encoding='utf8') as fp:\n",
    "    # Ignore POS contain '!'\n",
    "    for line in fp:\n",
    "        data = line.split()\n",
    "        idx += 1\n",
    "        if len(data[0])==1:\n",
    "            pos_tag = data[2] if data[2].find('!')<0 else data[2][1:]\n",
    "            pos_dict[data[0]].add(pos_tag)\n",
    "        else:            \n",
    "            break\n",
    "        \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ch_x, ch_y = '的','得'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pos_x = pos_dict[ch_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pos_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pos_y = pos_dict[ch_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pos_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "dd = {('Caa','Cab'):'C'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_simplify(pos_tags, level=1):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "    output = set()    \n",
    "    for p in pos_tags:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# same common\n",
    "if pos_y.intersection(pos_x):    \n",
    "    print('y')\n",
    "    \n",
    "# Simplified common \n",
    "elif :\n",
    "\n",
    "# basic common \n",
    "elif :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 取前兩碼，不足則全取\n",
    "2. Daa != Dab\n",
    "3. Nc != Ncd\n",
    "4. Neu, Nes, Nep, Neqa, Neqb 保留\n",
    "5. T 取一碼\n",
    "6. VA != VAC \n",
    "7. VH != VHC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''' pandas.Dataframe\n",
    "df = pd.read_csv(filelist['CKIP_Dictionary_UTF8.txt'], sep='\\t', header=None, names=['title','n1','pos','n2','pronunciation','def'])\n",
    "pos_df = df[df.apply(lambda x:len(x['title'])==1, axis=1)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "section_label = 'fasttext'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GOOD\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(filelist['UDN.doc.char.skipgram.vec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (UDN)- NOT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    with open(filepath, 'r') as fp:\n",
    "        for line in fp:\n",
    "            yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(N, tf, df):\n",
    "    return tf * math.log(N / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section_label = 'tfidf'\n",
    "filelist = dict((file,dataroot(section_label)+file) for file in os.listdir(dataroot(section_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compare function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fasttext_compare(ch_x, ch_y):\n",
    "    try:\n",
    "        return fasttext_model.similarity(ch_x, ch_y)\n",
    "    except KeyError:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_compare_SIGHAN(ch_x, ch_y):\n",
    "    '''\n",
    "    return (similar, 同部首同筆畫數)\n",
    "    '''\n",
    "    cands1 = shape_SIGHAN.get(ch_x, [])\n",
    "    try:\n",
    "        cands2 = sound_SIGHAN.loc[ch_x].同部首同筆畫數\n",
    "        if type(cands2)==float:\n",
    "            cands2 = []\n",
    "    except KeyError:\n",
    "        cands2 = []\n",
    "    \n",
    "    out1 = 1 if ch_y in cands1 else 0\n",
    "    out2 = 1 if ch_y in cands2 else 0\n",
    "    \n",
    "    return (out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_compare_SIGHAN(ch_x, ch_y):\n",
    "    '''\n",
    "    4. 同音同調\n",
    "    3. 同音異調\n",
    "    2. 近音同調\n",
    "    1. 近音異調\n",
    "    0 Not Found  \n",
    "    '''\n",
    "    try:\n",
    "        row = sound_SIGHAN.loc[ch_x]\n",
    "        for idx, col in enumerate(row[:-1]):\n",
    "            if type(col)==str and col.find(ch_y)!=-1:\n",
    "                return 4-idx\n",
    "                break\n",
    "        else:\n",
    "            return 0\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_compare_unihan(ch_x,ch_y):\n",
    "    if ch_y in sound_extract_same(ch_x):\n",
    "        return 4\n",
    "    elif ch_y in sound_extract_tone(ch_x):\n",
    "        return 3\n",
    "    elif ch_y in sound_extract_similartConsonant(ch_x, toneKeep=True):\n",
    "        return 2\n",
    "    elif ch_y in sound_extract_similartConsonant(ch_x, toneKeep=False):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def shape_compare_unihan(ch_x,ch_y):\n",
    "    if ch_y in shape_similar(ch_x):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cangjie_compare_unihan(ch_x,ch_y):\n",
    "    '''\n",
    "    Compare the cangjie between two character\n",
    "    applied LCS to check whether the two chars have similar cangjie code \n",
    "    '''\n",
    "    \n",
    "    def lcs(xstr, ystr):\n",
    "        \"\"\"\n",
    "        >>> lcs('thisisatest', 'testing123testing')\n",
    "        'tsitest'\n",
    "        \"\"\"\n",
    "        if not xstr or not ystr:\n",
    "            return \"\"\n",
    "        x, xs, y, ys = xstr[0], xstr[1:], ystr[0], ystr[1:]\n",
    "        if x == y:\n",
    "            return x + lcs(xs, ys)\n",
    "        else:\n",
    "            return max(lcs(xstr, ys), lcs(xs, ystr), key=len)\n",
    "    \n",
    "    cang_x = dicCangjie.get(ch_x,[])\n",
    "    cang_y = dicCangjie.get(ch_y,[])\n",
    "    \n",
    "    if len(cang_x)==0 or len(cang_y)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        cang_x == cang_x[0]\n",
    "        cang_y == cang_y[0]\n",
    "    \n",
    "    if cang_x == cang_y:\n",
    "        return 2\n",
    "    else:\n",
    "        lcs_length = len(lcs(cang_x, cang_y))\n",
    "        if len(cang_x) == 2:\n",
    "            if (lcs_length == 1 and len(cang_y)==2)\\\n",
    "            or (lcs_length == 2 and len(cang_y)==3):\n",
    "                return 1\n",
    "        elif len(cang_x) == 3:\n",
    "            if lcs_length == 2 and len(cang_y)<=4:\n",
    "                return 1\n",
    "        elif len(cang_x) == 4:\n",
    "            if lcs_length == 3 and len(cang_y)>=3:\n",
    "                return 1\n",
    "        elif len(cang_y) == 5:\n",
    "            if lcs_length == 4 and len(cang_y)==4:\n",
    "                return 1\n",
    "    \n",
    "    return 0     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compare between character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comparison4confusion(ch_chunk):\n",
    "    '''Similarity between two characters\n",
    "    * sound unihan \n",
    "    * shape unihan\n",
    "    * cangjie unihan\n",
    "    * sound SIGHAN\n",
    "    * shape SIGHAN\n",
    "    \n",
    "    * shape SIGHAN2\n",
    "    * frequency bigUnihan\n",
    "    * lm of ch_x\n",
    "    * lm of ch_y\n",
    "    \n",
    "    * fasttext \n",
    "    \n",
    "    * many,many confusion pairs\n",
    "    \n",
    "    Args:\n",
    "        ch_chunk (tuple,list): ch_chunk[0]=ch_x, ch_chunk[1]=ch_y\n",
    "    Return:\n",
    "        score (float): score of similariy \n",
    "        log (list): features of the similairy between two characters    \n",
    "    '''\n",
    "    \n",
    "    ch_x = ch_chunk[0]\n",
    "    ch_y = ch_chunk[1]\n",
    "    \n",
    "    log = list()\n",
    "    score = 0.0\n",
    "    \n",
    "    # 0. 4 3 2 1 0\n",
    "    tmp = sound_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "    \n",
    "    # 1. 1 0 \n",
    "    tmp = shape_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "    \n",
    "    # 2. 2 1 0 \n",
    "    tmp = cangjie_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "\n",
    "    # 3. 4 3 2 1 0\n",
    "    tmp = sound_compare_SIGHAN(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "\n",
    "    # 4. 1 0 / 1 0 \n",
    "    tmp = shape_compare_SIGHAN(ch_x,ch_y)\n",
    "    log.extend(tmp)\n",
    "    score = score + tmp[0] + tmp[1]\n",
    "    \n",
    "    # 5. 4 3 2 1 0\n",
    "    tmp = (5-dicFreq.get(ch_y,5))\n",
    "    log.append(tmp)\n",
    "    score += tmp        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 7. float \n",
    "    # log probability \n",
    "    tmp = lm.scoring(ch_x)\n",
    "    log.append(tmp)\n",
    "#     score -= tmp\n",
    "    \n",
    "    # 8. float \n",
    "    # log probability \n",
    "    tmp = lm.scoring(ch_y)\n",
    "    log.append(tmp)\n",
    "#     score -= tmp\n",
    "        \n",
    "    # 9. fasttext\n",
    "    # Probability \n",
    "    tmp = fasttext_compare(ch_x, ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp*10\n",
    "        \n",
    "    # Last\n",
    "    evidence = []\n",
    "    tmp = 0\n",
    "    for i in [confusion_pairs.items(), confusion_sentences.items()]:        \n",
    "        for filename, (ch_dict,_) in i:\n",
    "            if ch_y in ch_dict.get(ch_x,[]):\n",
    "                tmp += 1\n",
    "                evidence.append(filename)\n",
    "                score += 1        \n",
    "    log.append(tmp)\n",
    "    \n",
    "    # Last: Existed confusion pairs\n",
    "    log.append(evidence)    \n",
    "    \n",
    "    return (score,log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confusion_pairs['1新編常用錯別字門診.txt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# comparison4confusion(('事','事'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.102365290888805,\n",
       " [4, 0, 0, 4, 0, 0, 1.0, -3.102533, -3.774182, 0.41023652908888042, 0, []])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison4confusion(('世','氏'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractFeature(outputfilename, process_cnt, test=0):\n",
    "    ch_label = set(dicBPMF.keys()).union(set(sound_SIGHAN.index))\n",
    "    ch_n_label = random.choices(list(ch_label),k=test) if test>0 else ch_label    \n",
    "\n",
    "    bigDict = defaultdict(dict)\n",
    "\n",
    "    start_time = time.clock()\n",
    "    with multiprocessing.Pool(processes=process_cnt) as pool:\n",
    "        for ch_x in ch_n_label:\n",
    "            ch_n_inside = list(ch_n_label)\n",
    "            ch_n_inside.remove(ch_x)\n",
    "\n",
    "            ch_chunk = [(ch_x, ch_y) for ch_y in ch_n_inside]\n",
    "\n",
    "            scores = pool.map(comparison4confusion, ch_chunk)\n",
    "\n",
    "            for idx,(_,ch_y) in enumerate(ch_chunk):\n",
    "                if scores[idx][0]>5.0:\n",
    "                    bigDict[ch_x][ch_y] = scores[idx]\n",
    "\n",
    "    with open(outputfilename, 'wb') as fp:\n",
    "        pickle.dump(bigDict,fp)\n",
    "    \n",
    "    print(time.clock()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_command():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output', required=True)\n",
    "    parser.add_argument('--process', type=int, default=8)\n",
    "    parser.add_argument('--count', type=int, default=0)\n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    args = process_command()\n",
    "    \n",
    "    extractFeature(args.output, args.process, args.count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
