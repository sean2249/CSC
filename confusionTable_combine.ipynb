{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all confusion set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import xlrd\n",
    "import multiprocessing\n",
    "\n",
    "import random\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big unihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigUnihan_extract(filename):\n",
    "    df = pd.read_csv(filename, sep='|', low_memory=False)\n",
    "    df = df[['char','kFrequency']].set_index('char')\n",
    "    df = df[~pd.isnull(df.kFrequency)]\n",
    "    \n",
    "    return df.to_dict()['kFrequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIGHAN_char_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_compare_SIGHAN(ch_x, ch_y):\n",
    "    '''\n",
    "    return (similar, 同部首同筆畫數)\n",
    "    '''\n",
    "    cands1 = shape_SIGHAN.get(ch_x, [])\n",
    "    try:\n",
    "        cands2 = sound_SIGHAN.loc[ch_x].同部首同筆畫數\n",
    "        if type(cands2)==float:\n",
    "            cands2 = []\n",
    "    except KeyError:\n",
    "        cands2 = []\n",
    "    \n",
    "    out1 = 1 if ch_y in cands1 else 0\n",
    "    out2 = 1 if ch_y in cands2 else 0\n",
    "    \n",
    "    return (out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_compare_SIGHAN(ch_x, ch_y):\n",
    "    '''\n",
    "    4. 同音同調\n",
    "    3. 同音異調\n",
    "    2. 近音同調\n",
    "    1. 近音異調\n",
    "    0 Not Found  \n",
    "    '''\n",
    "    try:\n",
    "        row = sound_SIGHAN.loc[ch_x]\n",
    "        for idx, col in enumerate(row[:-1]):\n",
    "            if type(col)==str and col.find(ch_y)!=-1:\n",
    "                return 4-idx\n",
    "                break\n",
    "        else:\n",
    "            return 0\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "同音同調                                         湘像襄嚮鄉鑲橡瓖廂巷象香箱項向\n",
       "同音異調                               祥詳襄降翔鑲橡巷廂享餉響像湘嚮鄉庠瓖象饗香箱想項向\n",
       "近音同調       信心彊燼芯馨降筋縉漿進韁訢醬巾新津噤莘儘匠疆覲斤近昕今欣江鋅矜薑勁襟觔絳僵盡金晉禁僅薪浸辛姜釁將強\n",
       "近音異調       信彊心燼馨芯降筋縉漿謹進韁獎醬訢巾新噤津儘莘匠疆覲斤講槳近錦昕今欣江鋅矜薑勁襟觔瑾絳僵盡饉金...\n",
       "同部首同筆畫數                                              眉看省盼盾盹眇\n",
       "Name: 相, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_SIGHAN.loc['相']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_compare_SIGHAN('相','向')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unihan.csv (注音跟倉頡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unihan_extract(unihan_filename):\n",
    "    global dicBPMF, dicPhone, dicCangjie, dicCangjie2char\n",
    "    with open(unihan_filename, 'r', encoding='utf8') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in spamreader:\n",
    "            row = [cell for cell in row] # unicode\n",
    "            char, bpmf, cangjie, components, jp, kr, name, pinyin_chs, pinyin_cht, char_strokes_count, radical, radical_name, radical_strokes_count = row\n",
    "            for ph in bpmf.split(): # 發音\n",
    "                dicBPMF[char] += [ph]\n",
    "                dicPhone[ph] += [char]\n",
    "            for cj in cangjie.split(): # 倉頡碼\n",
    "                if u\"難\" in cj: continue\n",
    "                for i in range(0, 3):\n",
    "                    if i == len(cj): continue\n",
    "                    # dicBPMF[char]['cangjie'] += [cj[:i]]\n",
    "                    dicCangjie[char] += [cj[i:]]\n",
    "                    # dicCangjie[cj[:i]] += [char]\n",
    "                    dicCangjie2char[cj[i:]] += [char]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Unihan.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zwt.titles.txt (字典)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zwtTitle_train(lines):\n",
    "    d = defaultdict(lambda: 0)\n",
    "    for word in lines:\n",
    "        d[word.strip()] += 1\n",
    "    #d[word.strip().decode('utf-8')[:2]] += 1\n",
    "    #print word.strip().decode('utf-8')[:2]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## radical.txt (部首)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def radicalDic(lines):\n",
    "    dicRadicalnum = defaultdict(list)\n",
    "    dicRadical = defaultdict(list)\n",
    "    for line in lines:\n",
    "        for char in line[5:].strip().split('|'):\n",
    "            dicRadical[char] += [line[:4]]\n",
    "            dicRadicalnum[line[:4]] += [char]\n",
    "    return dicRadicalnum, dicRadical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_similar(char):\n",
    "    return list(set(ch for rnum in dicRadical[char] for ch in dicRadicalnum[rnum]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tone extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_extract_same(char):\n",
    "    '''\n",
    "    Same neutral and tone\n",
    "    '''\n",
    "    return list(set(ch for ph in dicBPMF[char] for ch in dicPhone[ph]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sound_extract_tone(char):\n",
    "    '''\n",
    "    the char of different tone \n",
    "    '''\n",
    "    output = set()\n",
    "    tones = ['ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    for ph in dicBPMF[char]:\n",
    "        if ph[-1] in tones:\n",
    "            for t in tones:\n",
    "                if t == ph[-1]: continue\n",
    "                output = output.union(dicPhone[ph[:-1]+t])\n",
    "        else:\n",
    "            for t in tones:\n",
    "                output = output.union(dicPhone[ph+t])\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sound_extract_finalConsonant(char, toneKeep=True):\n",
    "    '''\n",
    "    單：ㄚㄛㄜㄝ\n",
    "    複：ㄞㄟㄠㄡ\n",
    "    鼻：ㄢㄣㄤㄥ\n",
    "    捲舌：ㄦ\n",
    "    '''\n",
    "    output = set()\n",
    "    tones = ['','ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    consonants = [\n",
    "        ['ㄚ','ㄛ','ㄜ','ㄝ'],\n",
    "        ['ㄞ','ㄟ','ㄠ','ㄡ'],\n",
    "        ['ㄢ','ㄣ','ㄤ','ㄥ']\n",
    "    ]\n",
    "    \n",
    "    for ph in dicBPMF[char]:\n",
    "        # Tone delete\n",
    "        if ph[-1] in tones:\n",
    "            neutral, tone = ph[:-1], ph[-1]\n",
    "        else:\n",
    "            neutral, tone = ph,''\n",
    "        \n",
    "        # Add relevent consonant\n",
    "        for cons in consonants:\n",
    "            if neutral[-1] in cons:\n",
    "                new_neutrals = set(neutral[:-1] + c for c in cons if c!=neutral[-1])\n",
    "                for n in new_neutrals:\n",
    "                    if toneKeep:\n",
    "                        output = output.union(dicPhone[n+tone])\n",
    "                    else:\n",
    "                        for t in tones:\n",
    "                            output = output.union(dicPhone[n+t])\n",
    "                break\n",
    "                               \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_extract_similartConsonant(char, toneKeep=True):\n",
    "    '''\n",
    "    一次只針對一種，不會並用\n",
    "    Initial\n",
    "    ㄈㄏ\n",
    "    ㄋㄌ\n",
    "    ㄓㄗ\n",
    "    ㄔㄘ\n",
    "    Final:\n",
    "    ㄢㄤ\n",
    "    ㄜㄦ\n",
    "    ㄣㄥ\n",
    "    Intermediate:\n",
    "    ㄧㄩ\n",
    "    '''\n",
    "    new_neutrals = set()\n",
    "    output = set()\n",
    "    tones = ['','ˊ', 'ˇ', 'ˋ', '˙']\n",
    "    initial_pairs = [\n",
    "        ['ㄈ','ㄏ'],\n",
    "        ['ㄋ','ㄌ'],\n",
    "        ['ㄓ','ㄗ'],\n",
    "        ['ㄔ','ㄘ']\n",
    "    ]\n",
    "    final_pairs = [\n",
    "        ['ㄢ','ㄤ'],\n",
    "        ['ㄜ','ㄦ'],\n",
    "        ['ㄣ','ㄥ']\n",
    "    ]\n",
    "    inter_pairs = [['ㄧ','ㄩ']]\n",
    "    \n",
    "    for ph in dicBPMF[char]:        \n",
    "        # Tone delete\n",
    "        if ph[-1] in tones:\n",
    "            neutral, tone = ph[:-1], ph[-1]\n",
    "        else:\n",
    "            neutral, tone = ph, ''\n",
    "            \n",
    "        # Initial-consonant, just pick one \n",
    "        for cons in initial_pairs:\n",
    "            if neutral[0] in cons:\n",
    "                new_neutrals = new_neutrals.union(c + neutral[1:] + tone for c in cons if c!=neutral[0])\n",
    "                break        \n",
    "        \n",
    "                    \n",
    "        # Final-consonant       \n",
    "        for cons in final_pairs:\n",
    "#             print(neutral[-1], cons)\n",
    "            if neutral[-1] in cons:\n",
    "#                 print('i', cons)\n",
    "#                 print(neutral[:-1])\n",
    "#                 print(list(neutral[:-1] + c for c in cons if c!=neutral[-1]))\n",
    "                new_neutrals = new_neutrals.union(neutral[:-1] + c + tone for c in cons if c!=neutral[-1])\n",
    "                break\n",
    "        \n",
    "        # Inter_\n",
    "        for cons in inter_pairs:\n",
    "            for idx, tmp in enumerate(neutral):\n",
    "                if tmp in cons:\n",
    "                    new_neutrals = new_neutrals.union(neutral[:idx] + c + neutral[idx+1:] + tone for c in cons if c!=tmp)\n",
    "                    break\n",
    "    \n",
    "    ######## fIX TOne pRoblEm\n",
    "#     print(new_neutrals)\n",
    "    # Get candidate based on new_neutrals\n",
    "    for n in new_neutrals:\n",
    "        if toneKeep:\n",
    "            output = output.union(dicPhone[n])\n",
    "        else:\n",
    "            tmp = n[:-1] if n[-1] in tones else n            \n",
    "            for t in tones:\n",
    "                output = output.union(dicPhone[n+t])\n",
    "#         print(n,' '.join(output))\n",
    "        \n",
    "#     print(len(output))                     \n",
    "    return output if len(output)>0 else []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cangjie Extraction (from UNIHAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cangjie_extract_same(char):\n",
    "    cang = dicCangjie[char]\n",
    "    if len(cang) > 0:\n",
    "        output = set(dicCangjie2char[cang[0]])\n",
    "        output.remove(char)\n",
    "    else:\n",
    "        output = set()\n",
    "    \n",
    "    return list(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cangjie_compare_unihan(ch_x,ch_y):\n",
    "    '''\n",
    "    Compare the cangjie between two character\n",
    "    applied LCS to check whether the two chars have similar cangjie code \n",
    "    '''\n",
    "    \n",
    "    def lcs(xstr, ystr):\n",
    "        \"\"\"\n",
    "        >>> lcs('thisisatest', 'testing123testing')\n",
    "        'tsitest'\n",
    "        \"\"\"\n",
    "        if not xstr or not ystr:\n",
    "            return \"\"\n",
    "        x, xs, y, ys = xstr[0], xstr[1:], ystr[0], ystr[1:]\n",
    "        if x == y:\n",
    "            return x + lcs(xs, ys)\n",
    "        else:\n",
    "            return max(lcs(xstr, ys), lcs(xs, ystr), key=len)\n",
    "    \n",
    "    cang_x = dicCangjie.get(ch_x,[])\n",
    "    cang_y = dicCangjie.get(ch_y,[])\n",
    "    \n",
    "    if len(cang_x)==0 or len(cang_y)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        cang_x == cang_x[0]\n",
    "        cang_y == cang_y[0]\n",
    "    \n",
    "    if cang_x == cang_y:\n",
    "        return 2\n",
    "    else:\n",
    "        lcs_length = len(lcs(cang_x, cang_y))\n",
    "        if len(cang_x) == 2:\n",
    "            if (lcs_length == 1 and len(cang_y)==2)\\\n",
    "            or (lcs_length == 2 and len(cang_y)==3):\n",
    "                return 1\n",
    "        elif len(cang_x) == 3:\n",
    "            if lcs_length == 2 and len(cang_y)<=4:\n",
    "                return 1\n",
    "        elif len(cang_x) == 4:\n",
    "            if lcs_length == 3 and len(cang_y)>=3:\n",
    "                return 1\n",
    "        elif len(cang_y) == 5:\n",
    "            if lcs_length == 4 and len(cang_y)==4:\n",
    "                return 1\n",
    "    \n",
    "    return 0     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error_correct pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractPairs(filelist):\n",
    "    for filename, path in filelist.items():\n",
    "        print('== Filename: {}'.format(filename))\n",
    "        '''\n",
    "        QQQQQQ 有兩個以上的錯誤在一個詞裡面，但更正只有一項\n",
    "        把上方例子放棄不取\n",
    "        有 duplicate \n",
    "        '''\n",
    "        # 1新編常用錯別字門診.txt OR 4教育部錯別字表.txt\n",
    "        if filename.startswith('1') or filename.startswith('4'):\n",
    "            df = pd.read_csv(path, sep='\\t')\n",
    "        # 2東東錯別字.txt OR 3常見錯別字一覽表.txt\n",
    "        elif filename.startswith('2') or filename.startswith('3'):        \n",
    "            df = pd.read_csv(path, sep='\\t', header=None, names = ['正確詞','錯誤詞','正確字','錯誤字'])\n",
    "        elif filename.startswith('udn_common'):\n",
    "            table = xlrd.open_workbook(path).sheet_by_index(0)\n",
    "            ch_dict = defaultdict(set)\n",
    "            # Have multierros (error_word to correct_word)\n",
    "            word_dict = defaultdict(set)\n",
    "            for idx in range(1,table.nrows):\n",
    "                row = table.row_values(idx)[:5]\n",
    "                # Consider the priority of pairs \n",
    "                if row[2].strip():            \n",
    "                    chs = row[2].split()\n",
    "                    if len(chs)==1:\n",
    "                        continue\n",
    "                    for i in range(1,len(chs)):\n",
    "                        freq = row[1] if type(row[1])==float else 1.0                \n",
    "                        ch_dict[chs[i]].add((int(freq),chs[0]))\n",
    "                elif row[3].strip():\n",
    "                    corr_seq = row[3].strip()\n",
    "                    error_seq = row[4].strip()\n",
    "                    word_dict[error_seq] = corr_seq\n",
    "            yield (filename, ch_dict, word_dict)\n",
    "            continue\n",
    "        elif filename.startswith('udn_pairs'):\n",
    "            ch_dict = defaultdict(set)\n",
    "            with open(path, 'r', encoding='utf8') as fp:\n",
    "                for line in fp:\n",
    "                    tt = line.split()\n",
    "                    if int(tt[2])>10:\n",
    "                        ch_dict[tt[0]].add((int(tt[2]), tt[1]))\n",
    "                    \n",
    "            yield (filename, ch_dict, dict())\n",
    "            continue\n",
    "        \n",
    "        print(filename)\n",
    "        \n",
    "        # For 1,2,3,4\n",
    "        if len(df)>0:\n",
    "            df = df.dropna()\n",
    "            df['idx'] = df.apply(lambda x:x['錯誤詞'].find(x['錯誤字']), axis=1)\n",
    "            df['pair'] = tuple(zip(df['idx'], df['錯誤字']))\n",
    "            df['noMultiErrors'] = df.apply(lambda x:x['正確詞']==x['錯誤詞'].replace(x['錯誤字'],x['正確字']), axis=1)\n",
    "            \n",
    "            # Remove multi-errors for the lack of right answer \n",
    "            preCnt = len(df)\n",
    "            df = df[df['noMultiErrors']==True]\n",
    "            postCnt = len(df)\n",
    "            \n",
    "            print('Original:{}\\tPost:{}'.format(preCnt,postCnt))\n",
    "            \n",
    "            df = df.set_index('錯誤詞')\n",
    "            \n",
    "            # Output DICT{'error_word':'(idx, corr_ch)'}\n",
    "#             df_slice = df[['pair']]\n",
    "#             word_dict = df_slice.to_dict()['pair']\n",
    "            word_dict = df[['正確詞']].to_dict()['正確詞']\n",
    "\n",
    "            # output DICT{'error_ch':set(cands)}\n",
    "            ch_dict = defaultdict(lambda :set())\n",
    "            pairs = tuple(zip(df['錯誤字'], df['正確字']))\n",
    "            for error_ch, corr_ch in pairs:\n",
    "                ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "            yield (filename, ch_dict, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Sentnece (from SIGHAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bakeoff-2013 not work\n",
    "2. sequence error not append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractSentence(filelist):\n",
    "    for filename, path in filelist.items():\n",
    "        print('== Filename: {}'.format(filename))\n",
    "        \n",
    "        with open(path,'r',encoding='utf8') as fp:\n",
    "            soup = BeautifulSoup(fp, 'lxml')\n",
    "        \n",
    "        ch_dict = defaultdict(set)\n",
    "        word_dict = defaultdict(set)\n",
    "        seq_dict  = defaultdict(set)\n",
    "        \n",
    "        # Different label\n",
    "        if filename.startswith('Bakeoff'):\n",
    "            pass \n",
    "            ############## NOT FIX\n",
    "            for idx,element in enumerate(soup.find_all('DOC')):  \n",
    "                # Text\n",
    "                text = dict()\n",
    "                for pas in element.find('p').find_all('passage'):\n",
    "                    text[pas.get('id')] = pas.string\n",
    "\n",
    "                # Mistake\n",
    "                for mistake in element.find_all('mistake'):\n",
    "                    mis_id = mistake.get('id')\n",
    "                    mis_loc = mistake.get('location')\n",
    "                    mis_wrong = mistake.find('wrong').string.strip()\n",
    "                    mis_corr  = mistake.find('correction').string.strip()\n",
    "                    cur_seq = text.get(mis_id, '')\n",
    "\n",
    "                    pairs =  [(mis_wrong,idx,x,y) for idx, (x,y) in enumerate(zip(mis_wrong, mis_corr)) if x!=y]\n",
    "\n",
    "                    # error-corr\n",
    "                    for mis_wrong,idx,error_ch,corr_ch in pairs:\n",
    "                        # char-based\n",
    "                        ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "                        # word-based\n",
    "                        word_dict[mis_wrong].add((idx,corr_ch))\n",
    "\n",
    "        else:            \n",
    "            for idx,element in enumerate(soup.find_all('essay')):  \n",
    "                # Text\n",
    "                text = dict()\n",
    "                for pas in element.find('text').find_all('passage'):\n",
    "                    text[pas.get('id')] = pas.string\n",
    "\n",
    "                # Mistake\n",
    "                for mistake in element.find_all('mistake'):\n",
    "                    mis_id = mistake.get('id')\n",
    "                    mis_loc = mistake.get('location')\n",
    "                    mis_wrong = mistake.find('wrong').string.strip()\n",
    "                    mis_corr  = mistake.find('correction').string.strip()\n",
    "                    cur_seq = text.get(mis_id, '')\n",
    "\n",
    "                    pairs =  [(mis_wrong,idx,x,y) for idx, (x,y) in enumerate(zip(mis_wrong, mis_corr)) if x!=y]\n",
    "\n",
    "                    # error-corr\n",
    "                    for mis_wrong,idx,error_ch,corr_ch in pairs:\n",
    "                        # char-based\n",
    "                        ch_dict[error_ch].add(corr_ch)\n",
    "\n",
    "                        # word-based\n",
    "                        word_dict[mis_wrong].add((idx,corr_ch))\n",
    "\n",
    "                        # sequence-based \n",
    "                        ### Have problem with multiple errors in single word \n",
    "            #             seq_dict[cur_seq].add((int(mis_loc)-1,corr_ch))\n",
    "    \n",
    "        yield (filename, ch_dict, word_dict, seq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Char information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataroot = '/home/kiwi/udn_data/training_confusion/char_information/'\n",
    "filelist = dict((file,dataroot+file) for file in os.listdir(dataroot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "dicBPMF = defaultdict(list)\n",
    "dicPhone = defaultdict(list)\n",
    "dicCangjie = defaultdict(list)\n",
    "dicCangjie2char = defaultdict(list)\n",
    "unihan_extract(filelist['unihan.csv'])\n",
    "\n",
    "sound_SIGHAN = pd.read_csv(filelist['Bakeoff2013_CharacterSet_SimilarPronunciation.txt'], sep='\\t', index_col=0)\n",
    "shape_SIGHAN = pd.read_csv(filelist['Bakeoff2013_CharacterSet_SimilarShape.txt'], \\\n",
    "                           sep=',', index_col=0, names=['cands']).to_dict()['cands']\n",
    "\n",
    "voc = zwtTitle_train(open(filelist['zwt.titles.txt']).readlines())\n",
    "\n",
    "dicRadicalnum, dicRadical = radicalDic(open(filelist['radical.txt'], 'r', encoding='utf8').readlines())\n",
    "\n",
    "dicFreq = bigUnihan_extract(filelist['unihan_utf8_new.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Error_corr_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataroot = '/home/kiwi/udn_data/training_confusion/error_corr_pair/'\n",
    "filelist = dict((file,dataroot+file) for file in os.listdir(dataroot))\n",
    "# special = filelist.pop('udn_common.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Filename: 4教育部錯別字表.txt\n",
      "4教育部錯別字表.txt\n",
      "Original:490\tPost:470\n",
      "ch_dict:416\tword_dict:470\n",
      "\n",
      "== Filename: 3常見錯別字一覽表.txt\n",
      "3常見錯別字一覽表.txt\n",
      "Original:1364\tPost:1265\n",
      "ch_dict:803\tword_dict:1172\n",
      "\n",
      "== Filename: 1新編常用錯別字門診.txt\n",
      "1新編常用錯別字門診.txt\n",
      "Original:490\tPost:470\n",
      "ch_dict:416\tword_dict:470\n",
      "\n",
      "== Filename: udn_common.xls\n",
      "ch_dict:384\tword_dict:1057\n",
      "\n",
      "== Filename: udn_pairs.csv\n",
      "ch_dict:273\tword_dict:0\n",
      "\n",
      "== Filename: 2東東錯別字.txt\n",
      "2東東錯別字.txt\n",
      "Original:57924\tPost:38353\n",
      "ch_dict:3385\tword_dict:37478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_pairs = dict()\n",
    "# confusion_pairs[special] = extractPairs_udn\n",
    "for filename, ch_dict, word_dict in extractPairs(filelist):\n",
    "    print('ch_dict:{}\\tword_dict:{}\\n'.format(len(ch_dict),len(word_dict)))\n",
    "    confusion_pairs[filename] = (ch_dict,word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Error_corr_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Filename: SIGHAN15_CSC_B2_Training_utf8.sgml\n",
      "ch_dict:715\tword_dict:1478\n",
      "\n",
      "== Filename: C1_training.sgml\n",
      "ch_dict:237\tword_dict:369\n",
      "\n",
      "== Filename: Bakeoff2013_SampleSet_WithError_utf8.txt\n",
      "ch_dict:0\tword_dict:0\n",
      "\n",
      "== Filename: SIGHAN15_CSC_A2_Training.sgml\n",
      "ch_dict:521\tword_dict:794\n",
      "\n",
      "== Filename: B1_training_utf8.sgml\n",
      "ch_dict:1165\tword_dict:3608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataroot = '/home/kiwi/udn_data/training_confusion/error_corr_sentence/'\n",
    "filelist = dict((file,dataroot+file) for file in os.listdir(dataroot))\n",
    "unwated_file = filelist.pop('big5')\n",
    "\n",
    "confusion_sentences = dict()\n",
    "for filename, ch_dict, word_dict, seq_dict in extractSentence(filelist):\n",
    "    print('ch_dict:{}\\tword_dict:{}\\n'.format(len(ch_dict),len(word_dict)))\n",
    "    confusion_sentences[filename] = (ch_dict,word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion_training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sound_compare_unihan(ch_x,ch_y):\n",
    "    if ch_y in sound_extract_same(ch_x):\n",
    "        return 4\n",
    "    elif ch_y in sound_extract_tone(ch_x):\n",
    "        return 3\n",
    "    elif ch_y in sound_extract_similartConsonant(ch_x, toneKeep=True):\n",
    "        return 2\n",
    "    elif ch_y in sound_extract_similartConsonant(ch_x, toneKeep=False):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def shape_compare_unihan(ch_x,ch_y):\n",
    "    if ch_y in shape_similar(ch_x):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comparison4confusion(ch_chunk):\n",
    "    ch_x = ch_chunk[0]\n",
    "    ch_y = ch_chunk[1]\n",
    "    \n",
    "    log = list()\n",
    "    score = 0.0\n",
    "    \n",
    "    # MaxScore = 36 (pair/sentence count:5)\n",
    "    \n",
    "    tmp = sound_compare_unihan(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "#     print('1',score)\n",
    "    \n",
    "    tmp = shape_compare_unihan(ch_x,ch_y) * 3\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "#     print('2',score)\n",
    "    \n",
    "    tmp = cangjie_compare_unihan(ch_x,ch_y) * 2\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "#     print('3',score)\n",
    "\n",
    "    tmp = sound_compare_SIGHAN(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score += tmp\n",
    "#     print('4',score)\n",
    "\n",
    "    \n",
    "    tmp = shape_compare_SIGHAN(ch_x,ch_y)\n",
    "    log.append(tmp)\n",
    "    score = score + tmp[0]*2 + tmp[1] * 4\n",
    "#     print('5',score)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for filename, (ch_dict,_) in confusion_pairs.items():\n",
    "        if ch_y in ch_dict.get(ch_x,[]):\n",
    "            log.append(filename)\n",
    "            score += 1\n",
    "\n",
    "    for filename, (ch_dict,_) in confusion_sentences.items():\n",
    "        if ch_y in ch_dict.get(ch_x,[]):\n",
    "            log.append(filename)\n",
    "            score += 1\n",
    "            \n",
    "    tmp = (5.0-dicFreq.get(ch_y,5))\n",
    "    if score!=0:\n",
    "        score += tmp\n",
    "    log.append(tmp)\n",
    "\n",
    "    return (score,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comparison(ch_x, ch_y):\n",
    "    print('=== Comparison')\n",
    "    print(ch_x, ch_y)\n",
    "    '''\n",
    "    相似音的處理\n",
    "    sound_extract_same\n",
    "    sound_extract_tone\n",
    "    sound_extract_similartConsonant\n",
    "    sound_extract_finalConsonant\n",
    "\n",
    "    同音同調 同音異調 異音同調 異音異調\n",
    "    '''\n",
    "        \n",
    "        \n",
    "    print('\\n=== Sound similar (from unihan)')\n",
    "    print(sound_compare_unihan(ch_x,ch_y))\n",
    "    \n",
    "    '''\n",
    "    shape_similar\n",
    "    '''\n",
    "    print('\\n=== Shape similar (from radical)')\n",
    "    print(shape_compare_unihan(ch_x,ch_y))\n",
    "\n",
    "    '''\n",
    "    cangjie_compare\n",
    "    2 same cangjie code \n",
    "    1 similar cangjie code \n",
    "    0 nothing special\n",
    "    '''\n",
    "    print('\\n=== Cangjie (from unihan)')\n",
    "    print( cangjie_compare_unihan(ch_x,ch_y))\n",
    "        \n",
    "    '''\n",
    "    SIGHAN\n",
    "    '''\n",
    "    print('\\n=== SIGHAN Data (sound)')\n",
    "    print(sound_compare_SIGHAN(ch_x, ch_y))\n",
    "    \n",
    "\n",
    "    print('\\n=== SIGHAN Data (shape)')\n",
    "    print(shape_compare_SIGHAN(ch_x,ch_y))\n",
    "#     if t[0]==1:\n",
    "#         print('Similar shape')\n",
    "#     if t[1]==1:\n",
    "#         print('同部首同筆畫數')\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Error-correct pair \n",
    "    'filename': (ch_dict,word_dict)\n",
    "    '''\n",
    "    print('\\n=== Error-correct pair')\n",
    "    for filename, (ch_dict,_) in confusion_pairs.items():\n",
    "        if ch_y in ch_dict.get(ch_x,[]):\n",
    "            print(filename)\n",
    "\n",
    "    '''\n",
    "    Error-correct sentence \n",
    "    'filename': (ch_dict, word_dict)\n",
    "    '''\n",
    "    print('\\n=== Error-correct sentence')\n",
    "    for filename, (ch_dict,_) in confusion_sentences.items():\n",
    "        if ch_y in ch_dict.get(ch_x,[]):\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ch_x = '相'\n",
    "\n",
    "# ch_y = random.choice(list(rr))\n",
    "\n",
    "# comparison(ch_x,ch_y)\n",
    "\n",
    "# ch_x = random.choice(sound_SIGHAN.index)\n",
    "# ch_y = random.choice(sound_SIGHAN.index)\n",
    "\n",
    "# comparison(ch_x,ch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12344599999999772\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    ch_label = set(dicBPMF.keys()).union(set(sound_SIGHAN.index))\n",
    "\n",
    "    ch_n_label = random.choices(list(ch_label),k=50)\n",
    "\n",
    "    # %%timeit -n 1 -r 1\n",
    "\n",
    "    # pool_size=multiprocessing.cpu_count()\n",
    "\n",
    "    bigDict = defaultdict(dict)\n",
    "\n",
    "    start_time = time.clock()\n",
    "    with multiprocessing.Pool(processes=4) as pool:\n",
    "        for ch_x in ch_n_label:\n",
    "            ch_n_inside = list(ch_n_label)\n",
    "\n",
    "            ch_n_inside.remove(ch_x)\n",
    "\n",
    "            ch_chunk = [(ch_x, ch_y) for ch_y in ch_n_inside]\n",
    "\n",
    "            scores = pool.map(comparison4confusion, ch_chunk)\n",
    "\n",
    "            for idx,(_,ch_y) in enumerate(ch_chunk):\n",
    "                if scores[idx][0]>0.0:\n",
    "    #             if scores[idx] >= 0:\n",
    "                    bigDict[ch_x][ch_y] = scores[idx]\n",
    "            bigDict[ch_x][ch_x] = (30,[])\n",
    "\n",
    "#     with open('./tt.pkl', 'wb') as fp:\n",
    "#         pickle.dump(bigDict,fp)\n",
    "    \n",
    "    print(time.clock()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/kiwi/udn_data/training_confusion/confu.pkl','rb') as fp:\n",
    "    bigDict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "# Find best correct score \n",
    "# Prune (not much candidates)\n",
    "idx = 0\n",
    "for ch, candsValue in bigDict.items():\n",
    "#     print(ch, candsValue)\n",
    "    \n",
    "    idx += 1\n",
    "    \n",
    "    if idx%100==0: print(idx)\n",
    "#     if idx>1: break\n",
    "    total_score = sum(score for cand, (score, _) in candsValue.items())\n",
    "    \n",
    "    ttttt = candsValue.items()\n",
    "    tmp = sorted(candsValue.items(), key=lambda x:x[1][0], reverse=True)\n",
    "    tmp.insert(0, (ch,(35,[])))\n",
    "    \n",
    "    if total_score > 1000:\n",
    "        \n",
    "        # pick 20\n",
    "        \n",
    "        tmp = tmp[:20]\n",
    "        \n",
    "    elif total_score > 500:\n",
    "        # pick 10\n",
    "        tmp = tmp[:10]\n",
    "        \n",
    "    elif total_score > 300:\n",
    "        # pick 5\n",
    "        tmp = tmp[:5]\n",
    "    elif total_score >100:\n",
    "        # pick 3 \n",
    "        tmp = tmp[:3]\n",
    "    else:\n",
    "        # pick 1 \n",
    "        tmp = tmp[0:0]\n",
    "        \n",
    "    tmp_score  = sum(val for _,(val,_) in tmp)\n",
    "    confus[ch] = dict((cand, value/tmp_score) for cand, (value,_) in tmp)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     confus[ch] = dict((cand, score/total_score) for cand, (score,_) in candsValue.items())\n",
    "    \n",
    "#     print(confus[ch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confu = defaultdict(set)\n",
    "for ch, candsValue in bigDict.items():\n",
    "#     print(ch, candsValue)\n",
    "    \n",
    "    idx += 1\n",
    "    \n",
    "    if idx%100==0: print(idx)\n",
    "    if idx>3: break\n",
    "#     total_score = sum(score for cand, (score, _) in candsValue.items())\n",
    "    \n",
    "    print(candsValue)\n",
    "    ttttt = candsValue.items()\n",
    "    tmp = sorted(candsValue.items(), key=lambda x:x[1][0], reverse=True)\n",
    "    \n",
    "    print(tmp)\n",
    "    for cand, (cnt_score,_) in tmp:\n",
    "        print(cand, cnt_score)\n",
    "        if cnt_score>7:\n",
    "            confu[ch].add(cand)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set, {})"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('球', (10.0, [4, 0, 0, 0, (0, 0), 6.0])),\n",
       " ('求', (10.0, [4, 0, 0, 0, (0, 0), 6.0])),\n",
       " ('鰌', (7.0, [4, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('遒', (7.0, [4, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('崷', (7.0, [4, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('湭', (7.0, [4, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('酋', (7.0, [4, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('蝤', (7.0, [4, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('仇', (6.0, [4, 0, 0, 0, (0, 0), 2.0])),\n",
       " ('尊', (5.0, [0, 3, 0, 0, (0, 0), 2.0])),\n",
       " ('猶', (5.0, [0, 3, 0, 0, (0, 0), 2.0])),\n",
       " ('汓', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('釓', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('唒', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('浗', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('訄', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('叴', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('泅', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('賕', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('逑', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('蛷', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('頯', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('艽', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('肍', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('捄', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('絿', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('氽', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('皳', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('毬', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('傮', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('苬', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('盚', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('朹', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('慒', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('鼽', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('梂', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('俅', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('㞗', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('莍', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('紌', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('殏', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('厹', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('頄', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('鯄', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('扏', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('犰', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('裘', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('虯', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('鯦', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('璆', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('鰽', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('鮂', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('脙', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('銶', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('䊵', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('囚', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('虬', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('逎', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('觓', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('觩', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('訅', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('渞', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('巰', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('釚', (4.0, [4, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('蠤', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('禉', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('螑', (3.0, [3, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('趥', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('揂', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('猷', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('糗', (3.0, [3, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('搝', (3.0, [3, 0, 0, 0, (0, 0), 0.0])),\n",
       " ('媨', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('楢', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('緧', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('奠', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('偤', (3.0, [0, 3, 0, 0, (0, 0), 0.0])),\n",
       " ('輶', (3.0, [0, 3, 0, 0, (0, 0), 0.0]))]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
