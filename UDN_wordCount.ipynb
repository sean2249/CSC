{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if False:\n",
    "#     cnt_char = defaultdict(int)\n",
    "#     with open(seqfilename, 'r', encoding='utf8') as fp:\n",
    "#         for line in fp:\n",
    "#             lst = line.strip().split()\n",
    "#             for ch in lst:\n",
    "#                 cnt_char[ch] += 1\n",
    "\n",
    "#     pickle.dump(file=open('UDN_charCount.pkl', 'wb'), obj=cnt_char)\n",
    "\n",
    "#     sort_cnt_char = sorted(cnt_char.items(), key=lambda x:(x[1],len(x[0])), reverse=True )\n",
    "\n",
    "#     with open('UDN_charCount.txt', 'w', encoding='utf8') as wp:\n",
    "#         for c, cnt in sort_cnt_char:\n",
    "#             wp.write('{},{}\\n'.format(c,cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_char(seqfilename):\n",
    "    cnt_char = defaultdict(int)\n",
    "    with open(seqfilename, 'r', encoding='utf8') as fp:\n",
    "        for line in fp:\n",
    "            lst = line.strip().split()\n",
    "            for ch in lst:\n",
    "                cnt_char[ch] += 1\n",
    "    \n",
    "    return cnt_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqfilename = '/home/kiwi/udn_data/UDN.sentence.char.txt'\n",
    "# seqfilename = 'C:/Users/newslab/Desktop/UDN.sentence.char.txt'\n",
    "# seqfilename = 'G:/UDN/lm_data/UDN.sentence.char.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# char_frequency\n",
    "cnt_char = count_char(seqfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preErrorfilename = './extractUDN_new/all/all_preError.csv'\n",
    "errorPostfilename = './extractUDN_new/all/all_errorPost.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_preError = pd.read_csv(preErrorfilename, sep='\\t')\n",
    "df_postError = pd.read_csv(errorPostfilename, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_words = []\n",
    "post_words = []\n",
    "words = set()\n",
    "for _, row in df_preError.iterrows():\n",
    "    errorword = '{}{}'.format(row['pre'],row['error'])\n",
    "    corrword = '{}{}'.format(row['pre'], row['corr'])    \n",
    "    pre_words.append((errorword, corrword))\n",
    "    words.update([errorword, corrword])\n",
    "    \n",
    "for _, row in df_postError.iterrows():\n",
    "    errorword = '{}{}'.format(row['error'], row['post'])\n",
    "    corrword = '{}{}'.format(row['corr'], row['post'])\n",
    "    post_words.append((errorword, corrword))\n",
    "    words.update([errorword, corrword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(chunk):\n",
    "    return 1 if chunk in words else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_count = defaultdict(int)\n",
    "\n",
    "with open(seqfilename, 'r', encoding='utf8') as fp, multiprocessing.Pool(processes=2) as pool:\n",
    "#     for line in fp:\n",
    "#     if True:\n",
    "    for i in range(3000):\n",
    "        line = fp.readline()\n",
    "        line_str = ''.join(line.strip().split())\n",
    "        \n",
    "        search_lst = [''.join(search) for search in zip(line_str[:-1:], line_str[1::])]\n",
    "        tag = pool.map(batch, search_lst)\n",
    "        \n",
    "        tag_word = [search_lst[idx] for idx, t in enumerate(tag) if t==1]\n",
    "        \n",
    "        for w in tag_word:\n",
    "            words_count[w] += 1\n",
    "\n",
    "pickle.dump(file=open('UDN_wordscount.pkl','wb'), obj=words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search_str = \\\n",
    "# '今天 昨天 污染 汙染 佈局 布局 臺灣 台灣 秘書 祕書 記錄 紀錄 越來 愈來 志工 義工 週邊 周邊 週期 周期 佔率 占率\\\n",
    "#  規劃 規畫 計劃 計畫 部份 部分 比例 比率 發佈 發布 提昇 提升 身份 身分 每週 每周 上週 上周 市佔 市占 公佈 公布'\n",
    "\n",
    "# search_token = dict()\n",
    "# for i in search_str.split():\n",
    "#     search_token[i] = 0\n",
    "    \n",
    "# words_count = defaultdict(int)\n",
    "\n",
    "# with open(seqfilename, 'r', encoding='utf8') as fp:\n",
    "#     for line in fp:\n",
    "#         line_str = ''.join(line.strip().split())\n",
    "#         for search in zip(line_str[:-1:],line_str[1::]):\n",
    "#             s = ''.join(search)\n",
    "#             if s in search_token:\n",
    "#                 search_token[s] += 1\n",
    "\n",
    "# with open('errorpairCount.csv', 'w', encoding='utf8') as wp:\n",
    "#     for p, cnt in search_token.items():\n",
    "#         wp.write('{},{}\\n'.format(p,cnt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
