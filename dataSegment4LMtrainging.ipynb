{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import jieba\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runUdnSegment(dataroot, outputfile, action, pattern):\n",
    "    '''Extract UDN news, and ouput char-level segement file \n",
    "    \n",
    "    Need to revise pattern regular experission \n",
    "    \n",
    "    Args:\n",
    "        dataroot (str): the position of UDN news (recursive)\n",
    "        outputfile (str): the position of output file \n",
    "        action (dict): parameter for seperate and segmentation-level \n",
    "        pattern (rgex): pattern for kicking unwanted character \n",
    "    Return: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    sep_method = action.get('sep_method', 'comma')\n",
    "    seg_level = action.get('seg_level', 'char')\n",
    "    \n",
    "    if os.path.exists(outputfile): \n",
    "        print('Clean %s' %(outputfile))\n",
    "        os.remove(outputfile)\n",
    "    \n",
    "    pattern = re.compile('TEXT')\n",
    "    for dirPath, dirName, filelist in os.walk(dataroot, topdown=False):\n",
    "        if pattern.search(dirPath):        \n",
    "            print(dirPath)\n",
    "            for file in filelist:\n",
    "                inputfile = dirPath+'/'+file\n",
    "    #             print(inputfile)\n",
    "                with open(inputfile, 'rb') as fp:\n",
    "                    data = fp.read().decode('big5-hkscs', 'ignore')\n",
    "                    soup = BeautifulSoup(data)\n",
    "                    \n",
    "                content = contentExtract(soup)\n",
    "                seqs = seperateSeq(content, sep_method)\n",
    "                seqs_filter = filterSeq(seqs, pattern)\n",
    "                seg_string = transformSeq(seqs_filter, seg_level)\n",
    "                \n",
    "\n",
    "                with open(outputfile, 'a', encoding='utf8') as wp:\n",
    "                    wp.write(seg_string+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contentExtract(soup):\n",
    "    '''Extract the string content of website \n",
    "    Args:\n",
    "        soup (Beatutifulsoup): website \n",
    "    Return:\n",
    "        output (str): the string content of website\n",
    "    '''\n",
    "    output = ''\n",
    "    for pTxt in soup.find_all('p'):\n",
    "        res = ''\n",
    "        for tag_c in pTxt.contents:\n",
    "            try:\n",
    "                if tag_c.get('class')==1:\n",
    "                    res = res+tag_c.string\n",
    "            except:\n",
    "                res = res + tag_c\n",
    "        res = res.strip('.\\f\\n\\r\\t\\v')\n",
    "        if len(res)==0:\n",
    "            continue\n",
    "        output = output + res+'\\n'\n",
    "#         print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seperateSeq(content, sep_method):\n",
    "    '''Seperate string into sub-sentence\n",
    "    Args:\n",
    "        content (str): website content \n",
    "        sep_method (str): 'original' or 'comma'\n",
    "    Return:\n",
    "        output (list): sub-sentence\n",
    "    '''\n",
    "    output = []\n",
    "    if sep_method == 'comma':\n",
    "        pattern = re.compile('[，。！？]')\n",
    "        content = ''.join(content.split('\\n'))\n",
    "\n",
    "        pre_idx=0\n",
    "        for idx, ch in enumerate(content):\n",
    "            if pattern.search(ch):\n",
    "                tmp = content[pre_idx:idx+1]\n",
    "                output.append(tmp)\n",
    "                pre_idx = idx+1\n",
    "\n",
    "    #     print(pre_idx, len(seq))\n",
    "        if pre_idx<len(content):\n",
    "            tmp = content[pre_idx:len(content)]\n",
    "            output.append(tmp)\n",
    "    elif sep_method:\n",
    "        output = content.split('\\n')\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterSeq(lst, pattern):\n",
    "    '''Filter unwanted sequence based on pattern\n",
    "    Args:\n",
    "        lst (list): the list of website seperated content \n",
    "        pattern (rgex): the pattern we don't want \n",
    "    Return:\n",
    "        output (list): list after filter\n",
    "    '''\n",
    "    \n",
    "    output = [seq for seq in lst if not pattern.search(seq)]\n",
    "    \n",
    "    if not output:\n",
    "        return list()\n",
    "    \n",
    "    if output[0].find('】'): \n",
    "        _ = output.pop(0)\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transformSeq(seqs, seg_level):\n",
    "    '''filter the line existed Unwatned pattern, and seperate the char with \"space\"\n",
    "    Args:\n",
    "        seqs (list): list from website \n",
    "        seg_level: which lm-level ('word' OR 'char)\n",
    "    Return:\n",
    "        output (str): string which have been seperated by 'space'\n",
    "    '''\n",
    "    output = list()\n",
    "    if seg_level == 'word':\n",
    "        for seq in seqs:\n",
    "            segs = jieba.cut(seq)\n",
    "            output.append(' '.join(segs))            \n",
    "    elif seg_level == 'char':\n",
    "        for seq in seqs:\n",
    "            output.append(' '.join(seq))\n",
    "    return '\\n'.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runSinica(inputfile, outputfile):\n",
    "    with open(inputfile, 'r', encoding='utf8') as fp,\\\n",
    "    open(outputfile, 'w', encoding='utf8') as wp:\n",
    "        for line in fp:\n",
    "            data = line.strip('\\n').split(' ')\n",
    "            output = []\n",
    "            for item in data:\n",
    "                tmp = item.split('|')\n",
    "                if len(tmp)==2:\n",
    "                    output.append(tmp[0])\n",
    "\n",
    "            wp.write(' '.join(output))\n",
    "            wp.write('\\n')\n",
    "    #         print(' '.join(output))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean G:/UDN/lm_data/UDN_0507Train.txt\n",
      "G:/UDN/Files/20160610\\TEXT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Program Files\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file F:\\Program Files\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:/UDN/Files/20160611\\TEXT\n",
      "G:/UDN/Files/20160612\\TEXT\n",
      "G:/UDN/Files/20160613\\TEXT\n",
      "G:/UDN/Files/20160614\\TEXT\n",
      "G:/UDN/Files/20160615\\TEXT\n",
      "G:/UDN/Files/20160616\\TEXT\n",
      "G:/UDN/Files/20160617\\TEXT\n",
      "G:/UDN/Files/20160618\\TEXT\n",
      "G:/UDN/Files/20160619\\TEXT\n",
      "G:/UDN/Files/20160620\\TEXT\n",
      "G:/UDN/Files/20160621\\TEXT\n",
      "G:/UDN/Files/20160622\\TEXT\n",
      "G:/UDN/Files/20160623\\TEXT\n",
      "G:/UDN/Files/20160624\\TEXT\n",
      "G:/UDN/Files/20160625\\TEXT\n",
      "G:/UDN/Files/20160626\\TEXT\n",
      "G:/UDN/Files/20160627\\TEXT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Program Files\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"https://theinitium.com/art\r\n",
      "icle/20160313-culture-colu\r\n",
      "mn-liuzijie/\r\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:/UDN/Files/20160628\\TEXT\n",
      "G:/UDN/Files/20160629\\TEXT\n",
      "G:/UDN/Files/20160630\\TEXT\n",
      "G:/UDN/Files/20160701\\TEXT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-14418a33147c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     runUdnSegment(dataroot='G:/UDN/Files/', outputfile='G:/UDN/lm_data/UDN_0507Train.txt',\n\u001b[0;32m----> 8\u001b[0;31m                  action=par, pattern=ptn)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5bdbde47b92e>\u001b[0m in \u001b[0;36mrunUdnSegment\u001b[0;34m(dataroot, outputfile, action, pattern)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[1;31m#             print(inputfile)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'big5-hkscs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    ptn = re.compile('[A-Za-z0-9.\\s]')\n",
    "    par = {\n",
    "        'sep_method':'comma'\n",
    "        , 'seg_level':'char'}\n",
    "    \n",
    "    runUdnSegment(dataroot='G:/UDN/Files/', outputfile='G:/UDN/lm_data/UDN_0507Train.txt',\n",
    "                 action=par, pattern=ptn)\n",
    "    \n",
    "\n",
    "#     dataroot = '/home/kiwi/Documents/udn_data/Files/'\n",
    "#     runCharSegment(dataroot,outputfile,pattern)\n",
    "#     runWordSegment(dataroot,'./lm_data/seg_all.txt')\n",
    "#     inputfile = '/home/kiwi/udn_data/training/sinica.corpus.txt'\n",
    "#     runSinica(inputfile,'./lm_data/sinica_word.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
