{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import jieba\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runUdnSegment(dataroot, outputfile, action, unwanted_ptn):\n",
    "    '''Extract UDN news, and ouput char-level segement file \n",
    "    \n",
    "    Need to revise pattern regular experission \n",
    "    \n",
    "    Args:\n",
    "        dataroot (str): the position of UDN news (recursive)\n",
    "        outputfile (str): the position of output file \n",
    "        action (dict): parameter for seperate and segmentation-level \n",
    "        unwanted_ptn (rgex): pattern for kicking unwanted character \n",
    "    Return: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    sep_method = action.get('sep_method', 'comma')\n",
    "    seg_level = action.get('seg_level', 'char')\n",
    "    \n",
    "    if os.path.exists(outputfile): \n",
    "        print('Clean %s' %(outputfile))\n",
    "        os.remove(outputfile)\n",
    "    \n",
    "    pattern = re.compile('TEXT')\n",
    "    for dirPath, dirName, filelist in os.walk(dataroot, topdown=False):\n",
    "        if pattern.search(dirPath):        \n",
    "            print(dirPath)\n",
    "            for file in filelist:\n",
    "                inputfile = dirPath+'/'+file\n",
    "    #             print(inputfile)\n",
    "                with open(inputfile, 'rb') as fp:\n",
    "                    data = fp.read().decode('big5-hkscs', 'ignore')\n",
    "                    soup = BeautifulSoup(data, 'lxml')\n",
    "                    \n",
    "                content = contentExtract(soup)\n",
    "                seqs = seperateSeq(content, sep_method)\n",
    "                seqs_filter = filterSeq(seqs, unwanted_ptn)\n",
    "                seg_string = transformSeq(seqs_filter, seg_level)\n",
    "                \n",
    "                with open(outputfile, 'a', encoding='utf8') as wp:\n",
    "                    wp.write(seg_string+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contentExtract(soup):\n",
    "    '''Extract the string content of website \n",
    "    Args:\n",
    "        soup (Beatutifulsoup): website \n",
    "    Return:\n",
    "        output (str): the string content of website\n",
    "    '''\n",
    "    output = ''\n",
    "    for pTxt in soup.find_all('p'):\n",
    "        res = ''\n",
    "        for tag_c in pTxt.contents:\n",
    "            try:\n",
    "                if tag_c.get('class')==1:\n",
    "                    res = res+tag_c.string\n",
    "            except:\n",
    "                res = res + tag_c\n",
    "        res = res.strip('.\\f\\n\\r\\t\\v')\n",
    "        if len(res)==0:\n",
    "            continue\n",
    "        output = output + res+'\\n'\n",
    "#         print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seperateSeq(content, sep_method):\n",
    "    '''Seperate string into sub-sentence\n",
    "    Args:\n",
    "        content (str): website content \n",
    "        sep_method (str): 'original' or 'comma'\n",
    "    Return:\n",
    "        output (list): sub-sentence\n",
    "    '''\n",
    "    output = []\n",
    "    if sep_method == 'comma':\n",
    "        pattern = re.compile('[，。！？]')\n",
    "        content = ''.join(content.split('\\n'))\n",
    "\n",
    "        pre_idx=0\n",
    "        for idx, ch in enumerate(content):\n",
    "            if pattern.search(ch):\n",
    "                tmp = content[pre_idx:idx+1]\n",
    "                output.append(tmp)\n",
    "                pre_idx = idx+1\n",
    "\n",
    "    #     print(pre_idx, len(seq))\n",
    "        if pre_idx<len(content):\n",
    "            tmp = content[pre_idx:len(content)]\n",
    "            output.append(tmp)\n",
    "    elif sep_method:\n",
    "        output = content.split('\\n')\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterSeq(lst, pattern):\n",
    "    '''Filter unwanted sequence based on pattern\n",
    "    Args:\n",
    "        lst (list): the list of website seperated content \n",
    "        pattern (rgex): the pattern we don't want \n",
    "    Return:\n",
    "        output (list): list after filter\n",
    "    '''\n",
    "    \n",
    "    output = [seq for seq in lst if not pattern.search(seq)]\n",
    "    \n",
    "    if not output:\n",
    "        return list()\n",
    "    \n",
    "    if output[0].find('】'): \n",
    "        _ = output.pop(0)\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transformSeq(seqs, seg_level):\n",
    "    '''filter the line existed Unwatned pattern, and seperate the char with \"space\"\n",
    "    Args:\n",
    "        seqs (list): list from website \n",
    "        seg_level: which lm-level ('word' OR 'char)\n",
    "    Return:\n",
    "        output (str): string which have been seperated by 'space'\n",
    "    '''\n",
    "    output = list()\n",
    "    if seg_level == 'word':\n",
    "        for seq in seqs:\n",
    "            segs = jieba.cut(seq)\n",
    "            output.append(' '.join(segs))            \n",
    "    elif seg_level == 'char':\n",
    "        for seq in seqs:\n",
    "            output.append(' '.join(seq))\n",
    "    return '\\n'.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runSinica(inputfile, outputfile):\n",
    "    with open(inputfile, 'r', encoding='utf8') as fp,\\\n",
    "    open(outputfile, 'w', encoding='utf8') as wp:\n",
    "        for line in fp:\n",
    "            data = line.strip('\\n').split(' ')\n",
    "            output = []\n",
    "            for item in data:\n",
    "                tmp = item.split('|')\n",
    "                if len(tmp)==2:\n",
    "                    output.append(tmp[0])\n",
    "\n",
    "            wp.write(' '.join(output))\n",
    "            wp.write('\\n')\n",
    "    #         print(' '.join(output))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean G:/UDN/lm_data/UDN_0507Train.txt\n",
      "G:/UDN/Files/20160610\\TEXT\n",
      "花 蓮 員 警 跨 縣 救 人 。\n",
      "【 王 思 慧 】 ● 馬 拉 松 選 手 熱 中 暑 ，\n",
      "醫 院 救 回 。\n",
      "【 羅 建 旺 】\n",
      "【 王 思 慧 】\n",
      "G:/UDN/Files/20160611\\TEXT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Program Files\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file F:\\Program Files\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    ptn = re.compile('[A-Za-z0-9.\\s]')\n",
    "    par = {\n",
    "        'sep_method':'comma'\n",
    "        , 'seg_level':'char'}\n",
    "    \n",
    "    data_root = sys.argv[1]\n",
    "    output_file = sys.argv[2]  \n",
    "    \n",
    "    runUdnSegment(dataroot=data_root, outputfile=output_file,\n",
    "                 action=par, unwanted_ptn=ptn)\n",
    "    \n",
    "\n",
    "#     dataroot = '/home/kiwi/Documents/udn_data/Files/'\n",
    "#     runCharSegment(dataroot,outputfile,pattern)\n",
    "#     runWordSegment(dataroot,'./lm_data/seg_all.txt')\n",
    "#     inputfile = '/home/kiwi/udn_data/training/sinica.corpus.txt'\n",
    "#     runSinica(inputfile,'./lm_data/sinica_word.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
